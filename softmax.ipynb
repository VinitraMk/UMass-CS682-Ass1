{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " # Softmax exercise\n",
    "\n",
    " *Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](https://compsci682-fa19.github.io/assignments2019/assignment1/) on the course website.*\n",
    "\n",
    " This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    " - implement a fully-vectorized **loss function** for the Softmax classifier\n",
    " - implement the fully-vectorized expression for its **analytic gradient**\n",
    " - **check your implementation** with numerical gradient\n",
    " - use a validation set to **tune the learning rate and regularization** strength\n",
    " - **optimize** the loss function with **SGD**\n",
    " - **visualize** the final learned weights\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "from cs682.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs682/datasets/cifar-10-batches-py'\n",
    "    # Don't forget to run get_datasets.sh, or this will throw an error\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "\n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "\n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Softmax Classifier\n",
    "\n",
    " Your code for this section will all be written inside **cs682/classifiers/softmax.py**.\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs682/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs682.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss: 2.358851\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Inline Question 1:\n",
    " Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    " **Your answer:** Initially when weights are randomly assigned, all classes\n",
    " are equally likely to be predicted by the classifier. The model has not\n",
    " begun learning parameters and generalized the data. So the probability\n",
    " of a class getting predicted by the model is 1/10 = 0.1. As the model starts to\n",
    " learn parameters, it generalizes the data and adjusts the weights depending their\n",
    " presence in the dataset. So the probability of a class could increase or decrease.\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs682.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numerical: 0.933195 analytic: 0.933195, relative error: 1.135139e-08\n",
      "numerical: -0.164618 analytic: -0.164618, relative error: 1.131511e-07\n",
      "numerical: -0.726051 analytic: -0.726051, relative error: 2.307141e-08\n",
      "numerical: -4.226875 analytic: -4.226875, relative error: 5.297797e-09\n",
      "numerical: 0.091942 analytic: 0.091942, relative error: 1.011601e-07\n",
      "numerical: -2.299650 analytic: -2.299650, relative error: 1.318220e-08\n",
      "numerical: -1.985016 analytic: -1.985016, relative error: 4.508728e-08\n",
      "numerical: -0.349872 analytic: -0.349872, relative error: 9.237913e-08\n",
      "numerical: -0.318365 analytic: -0.318365, relative error: 2.389062e-07\n",
      "numerical: -0.412830 analytic: -0.412830, relative error: 1.142612e-07\n",
      "numerical: 3.250537 analytic: 3.250537, relative error: 7.408727e-09\n",
      "numerical: 2.995079 analytic: 2.995079, relative error: 2.601999e-08\n",
      "numerical: -0.100293 analytic: -0.100293, relative error: 4.512738e-07\n",
      "numerical: -3.151161 analytic: -3.151161, relative error: 8.207694e-09\n",
      "numerical: -4.560104 analytic: -4.560104, relative error: 1.458063e-09\n",
      "numerical: 0.060662 analytic: 0.060661, relative error: 1.212364e-06\n",
      "numerical: 0.988585 analytic: 0.988585, relative error: 3.779339e-08\n",
      "numerical: 0.551180 analytic: 0.551180, relative error: 6.684949e-08\n",
      "numerical: 0.515142 analytic: 0.515142, relative error: 1.340126e-08\n",
      "numerical: -0.340357 analytic: -0.340357, relative error: 4.782919e-08\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs682.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "naive loss: 2.358851e+00 computed in 0.410119s\n",
      "vectorized loss: 2.358851e+00 computed in 0.008605s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs682.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "best_combo=()\n",
    "learning_rates = [1e-7, 2e-7, 3e-7, 4e-7, 5e-7, 1e-6, 2e-6, 3e-6, 4e-6, 5e-6, 1e-5, 2e-5, 3e-5, 4e-5, 5e-5]\n",
    "regularization_strengths = [1e3, 2e3, 3e3, 4e3, 5e3, 1e4, 2e4, 2.5e4, 3e4, 3.5e4, 4e4, 4.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "# Your code\n",
    "combos = [(v[0], v[1]) for v in list(itertools.product(learning_rates, regularization_strengths))]\n",
    "for combo in combos:\n",
    "    print('\\nEvaluating combo: ', combo)\n",
    "    softmax = Softmax()\n",
    "    loss_hist = softmax.train(X_train, y_train, learning_rate=combo[0], reg=combo[1], num_iters=1500,\n",
    "                        verbose=True)\n",
    "    \n",
    "    y_train_pred = softmax.predict(X_train)\n",
    "    y_val_pred = softmax.predict(X_val)\n",
    "    train_accuracy = np.mean(y_train == y_train_pred)\n",
    "    val_accuracy = np.mean(y_val == y_val_pred)\n",
    "    if best_val < val_accuracy:\n",
    "        best_val = val_accuracy\n",
    "        best_softmax  = softmax\n",
    "        best_combo = combo\n",
    "    results[combo] = (train_accuracy, val_accuracy)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "\n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "\n",
    "print(f'\\nbest validation accuracy achieved during cross-validation: {best_val} and the alpha, regularization that achieved it: {best_combo}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluating combo:  (1e-07, 1000.0)\n",
      "iteration 0 / 1500: loss 36.481212\n",
      "iteration 100 / 1500: loss 34.018574\n",
      "iteration 200 / 1500: loss 32.394887\n",
      "iteration 300 / 1500: loss 30.724795\n",
      "iteration 400 / 1500: loss 29.632422\n",
      "iteration 500 / 1500: loss 28.326805\n",
      "iteration 600 / 1500: loss 27.422426\n",
      "iteration 700 / 1500: loss 26.280433\n",
      "iteration 800 / 1500: loss 24.993368\n",
      "iteration 900 / 1500: loss 23.930709\n",
      "iteration 1000 / 1500: loss 23.086741\n",
      "iteration 1100 / 1500: loss 22.267685\n",
      "iteration 1200 / 1500: loss 21.376493\n",
      "iteration 1300 / 1500: loss 20.659859\n",
      "iteration 1400 / 1500: loss 20.261635\n",
      "\n",
      "Evaluating combo:  (1e-07, 2000.0)\n",
      "iteration 0 / 1500: loss 66.683274\n",
      "iteration 100 / 1500: loss 60.164602\n",
      "iteration 200 / 1500: loss 55.060900\n",
      "iteration 300 / 1500: loss 50.583058\n",
      "iteration 400 / 1500: loss 46.768259\n",
      "iteration 500 / 1500: loss 43.312425\n",
      "iteration 600 / 1500: loss 40.017410\n",
      "iteration 700 / 1500: loss 36.917123\n",
      "iteration 800 / 1500: loss 34.005682\n",
      "iteration 900 / 1500: loss 31.777506\n",
      "iteration 1000 / 1500: loss 29.557922\n",
      "iteration 1100 / 1500: loss 27.274381\n",
      "iteration 1200 / 1500: loss 25.155644\n",
      "iteration 1300 / 1500: loss 23.281546\n",
      "iteration 1400 / 1500: loss 21.652947\n",
      "\n",
      "Evaluating combo:  (1e-07, 3000.0)\n",
      "iteration 0 / 1500: loss 96.487582\n",
      "iteration 100 / 1500: loss 85.143780\n",
      "iteration 200 / 1500: loss 75.385599\n",
      "iteration 300 / 1500: loss 66.769239\n",
      "iteration 400 / 1500: loss 59.078610\n",
      "iteration 500 / 1500: loss 52.634570\n",
      "iteration 600 / 1500: loss 46.842970\n",
      "iteration 700 / 1500: loss 41.493506\n",
      "iteration 800 / 1500: loss 37.017761\n",
      "iteration 900 / 1500: loss 32.903301\n",
      "iteration 1000 / 1500: loss 29.350961\n",
      "iteration 1100 / 1500: loss 26.352125\n",
      "iteration 1200 / 1500: loss 23.477847\n",
      "iteration 1300 / 1500: loss 20.963037\n",
      "iteration 1400 / 1500: loss 18.870373\n",
      "\n",
      "Evaluating combo:  (1e-07, 4000.0)\n",
      "iteration 0 / 1500: loss 128.306550\n",
      "iteration 100 / 1500: loss 107.865406\n",
      "iteration 200 / 1500: loss 91.461840\n",
      "iteration 300 / 1500: loss 77.990678\n",
      "iteration 400 / 1500: loss 66.637415\n",
      "iteration 500 / 1500: loss 56.810415\n",
      "iteration 600 / 1500: loss 48.601197\n",
      "iteration 700 / 1500: loss 41.739541\n",
      "iteration 800 / 1500: loss 35.789008\n",
      "iteration 900 / 1500: loss 30.914341\n",
      "iteration 1000 / 1500: loss 26.342294\n",
      "iteration 1100 / 1500: loss 22.586137\n",
      "iteration 1200 / 1500: loss 19.643946\n",
      "iteration 1300 / 1500: loss 16.921991\n",
      "iteration 1400 / 1500: loss 14.733431\n",
      "\n",
      "Evaluating combo:  (1e-07, 5000.0)\n",
      "iteration 0 / 1500: loss 157.689104\n",
      "iteration 100 / 1500: loss 128.160438\n",
      "iteration 200 / 1500: loss 104.869737\n",
      "iteration 300 / 1500: loss 86.132921\n",
      "iteration 400 / 1500: loss 70.670671\n",
      "iteration 500 / 1500: loss 57.835569\n",
      "iteration 600 / 1500: loss 47.678990\n",
      "iteration 700 / 1500: loss 39.278759\n",
      "iteration 800 / 1500: loss 32.572378\n",
      "iteration 900 / 1500: loss 26.842212\n",
      "iteration 1000 / 1500: loss 22.358104\n",
      "iteration 1100 / 1500: loss 18.637208\n",
      "iteration 1200 / 1500: loss 15.501268\n",
      "iteration 1300 / 1500: loss 13.047219\n",
      "iteration 1400 / 1500: loss 10.972045\n",
      "\n",
      "Evaluating combo:  (1e-07, 10000.0)\n",
      "iteration 0 / 1500: loss 317.511407\n",
      "iteration 100 / 1500: loss 212.196678\n",
      "iteration 200 / 1500: loss 142.518287\n",
      "iteration 300 / 1500: loss 95.760074\n",
      "iteration 400 / 1500: loss 64.776279\n",
      "iteration 500 / 1500: loss 43.941686\n",
      "iteration 600 / 1500: loss 30.007505\n",
      "iteration 700 / 1500: loss 20.774785\n",
      "iteration 800 / 1500: loss 14.584427\n",
      "iteration 900 / 1500: loss 10.370073\n",
      "iteration 1000 / 1500: loss 7.562830\n",
      "iteration 1100 / 1500: loss 5.669970\n",
      "iteration 1200 / 1500: loss 4.548802\n",
      "iteration 1300 / 1500: loss 3.677384\n",
      "iteration 1400 / 1500: loss 3.060315\n",
      "\n",
      "Evaluating combo:  (1e-07, 20000.0)\n",
      "iteration 0 / 1500: loss 618.117907\n",
      "iteration 100 / 1500: loss 276.933090\n",
      "iteration 200 / 1500: loss 125.022110\n",
      "iteration 300 / 1500: loss 57.045127\n",
      "iteration 400 / 1500: loss 26.670787\n",
      "iteration 500 / 1500: loss 12.961696\n",
      "iteration 600 / 1500: loss 6.969271\n",
      "iteration 700 / 1500: loss 4.294574\n",
      "iteration 800 / 1500: loss 3.005897\n",
      "iteration 900 / 1500: loss 2.512304\n",
      "iteration 1000 / 1500: loss 2.255589\n",
      "iteration 1100 / 1500: loss 2.147891\n",
      "iteration 1200 / 1500: loss 2.145786\n",
      "iteration 1300 / 1500: loss 2.102707\n",
      "iteration 1400 / 1500: loss 2.079309\n",
      "\n",
      "Evaluating combo:  (1e-07, 25000.0)\n",
      "iteration 0 / 1500: loss 764.544338\n",
      "iteration 100 / 1500: loss 280.205676\n",
      "iteration 200 / 1500: loss 103.861169\n",
      "iteration 300 / 1500: loss 39.337835\n",
      "iteration 400 / 1500: loss 15.679045\n",
      "iteration 500 / 1500: loss 7.061930\n",
      "iteration 600 / 1500: loss 3.924374\n",
      "iteration 700 / 1500: loss 2.727655\n",
      "iteration 800 / 1500: loss 2.345125\n",
      "iteration 900 / 1500: loss 2.163846\n",
      "iteration 1000 / 1500: loss 2.115539\n",
      "iteration 1100 / 1500: loss 2.046920\n",
      "iteration 1200 / 1500: loss 2.092649\n",
      "iteration 1300 / 1500: loss 2.086217\n",
      "iteration 1400 / 1500: loss 2.045481\n",
      "\n",
      "Evaluating combo:  (1e-07, 30000.0)\n",
      "iteration 0 / 1500: loss 921.832687\n",
      "iteration 100 / 1500: loss 277.129517\n",
      "iteration 200 / 1500: loss 84.356321\n",
      "iteration 300 / 1500: loss 26.694773\n",
      "iteration 400 / 1500: loss 9.542829\n",
      "iteration 500 / 1500: loss 4.323225\n",
      "iteration 600 / 1500: loss 2.817846\n",
      "iteration 700 / 1500: loss 2.278802\n",
      "iteration 800 / 1500: loss 2.175758\n",
      "iteration 900 / 1500: loss 2.096852\n",
      "iteration 1000 / 1500: loss 2.112415\n",
      "iteration 1100 / 1500: loss 2.154904\n",
      "iteration 1200 / 1500: loss 2.130950\n",
      "iteration 1300 / 1500: loss 2.068661\n",
      "iteration 1400 / 1500: loss 2.122606\n",
      "\n",
      "Evaluating combo:  (1e-07, 35000.0)\n",
      "iteration 0 / 1500: loss 1067.018071\n",
      "iteration 100 / 1500: loss 262.645122\n",
      "iteration 200 / 1500: loss 65.835436\n",
      "iteration 300 / 1500: loss 17.729549\n",
      "iteration 400 / 1500: loss 5.970620\n",
      "iteration 500 / 1500: loss 3.033774\n",
      "iteration 600 / 1500: loss 2.310178\n",
      "iteration 700 / 1500: loss 2.104671\n",
      "iteration 800 / 1500: loss 2.085636\n",
      "iteration 900 / 1500: loss 2.112995\n",
      "iteration 1000 / 1500: loss 2.071788\n",
      "iteration 1100 / 1500: loss 2.141426\n",
      "iteration 1200 / 1500: loss 2.074632\n",
      "iteration 1300 / 1500: loss 2.133326\n",
      "iteration 1400 / 1500: loss 2.094023\n",
      "\n",
      "Evaluating combo:  (1e-07, 40000.0)\n",
      "iteration 0 / 1500: loss 1224.560982\n",
      "iteration 100 / 1500: loss 246.331784\n",
      "iteration 200 / 1500: loss 51.072284\n",
      "iteration 300 / 1500: loss 11.896975\n",
      "iteration 400 / 1500: loss 4.056296\n",
      "iteration 500 / 1500: loss 2.513527\n",
      "iteration 600 / 1500: loss 2.177472\n",
      "iteration 700 / 1500: loss 2.143703\n",
      "iteration 800 / 1500: loss 2.089413\n",
      "iteration 900 / 1500: loss 2.133043\n",
      "iteration 1000 / 1500: loss 2.128341\n",
      "iteration 1100 / 1500: loss 2.115384\n",
      "iteration 1200 / 1500: loss 2.161668\n",
      "iteration 1300 / 1500: loss 2.188137\n",
      "iteration 1400 / 1500: loss 2.097900\n",
      "\n",
      "Evaluating combo:  (1e-07, 45000.0)\n",
      "iteration 0 / 1500: loss 1384.733916\n",
      "iteration 100 / 1500: loss 227.935472\n",
      "iteration 200 / 1500: loss 39.082459\n",
      "iteration 300 / 1500: loss 8.178854\n",
      "iteration 400 / 1500: loss 3.118299\n",
      "iteration 500 / 1500: loss 2.286645\n",
      "iteration 600 / 1500: loss 2.143346\n",
      "iteration 700 / 1500: loss 2.166275\n",
      "iteration 800 / 1500: loss 2.111553\n",
      "iteration 900 / 1500: loss 2.098553\n",
      "iteration 1000 / 1500: loss 2.125649\n",
      "iteration 1100 / 1500: loss 2.083477\n",
      "iteration 1200 / 1500: loss 2.151727\n",
      "iteration 1300 / 1500: loss 2.085892\n",
      "iteration 1400 / 1500: loss 2.165020\n",
      "\n",
      "Evaluating combo:  (1e-07, 50000.0)\n",
      "iteration 0 / 1500: loss 1546.073541\n",
      "iteration 100 / 1500: loss 208.247866\n",
      "iteration 200 / 1500: loss 29.678782\n",
      "iteration 300 / 1500: loss 5.838972\n",
      "iteration 400 / 1500: loss 2.620440\n",
      "iteration 500 / 1500: loss 2.224410\n",
      "iteration 600 / 1500: loss 2.134509\n",
      "iteration 700 / 1500: loss 2.146625\n",
      "iteration 800 / 1500: loss 2.168422\n",
      "iteration 900 / 1500: loss 2.164906\n",
      "iteration 1000 / 1500: loss 2.092308\n",
      "iteration 1100 / 1500: loss 2.111569\n",
      "iteration 1200 / 1500: loss 2.094917\n",
      "iteration 1300 / 1500: loss 2.174486\n",
      "iteration 1400 / 1500: loss 2.160665\n",
      "\n",
      "Evaluating combo:  (2e-07, 1000.0)\n",
      "iteration 0 / 1500: loss 36.038698\n",
      "iteration 100 / 1500: loss 32.033608\n",
      "iteration 200 / 1500: loss 29.036080\n",
      "iteration 300 / 1500: loss 26.948519\n",
      "iteration 400 / 1500: loss 24.561258\n",
      "iteration 500 / 1500: loss 23.006670\n",
      "iteration 600 / 1500: loss 21.013418\n",
      "iteration 700 / 1500: loss 19.808431\n",
      "iteration 800 / 1500: loss 18.148423\n",
      "iteration 900 / 1500: loss 17.039283\n",
      "iteration 1000 / 1500: loss 15.679857\n",
      "iteration 1100 / 1500: loss 14.568982\n",
      "iteration 1200 / 1500: loss 13.657937\n",
      "iteration 1300 / 1500: loss 12.569554\n",
      "iteration 1400 / 1500: loss 11.725664\n",
      "\n",
      "Evaluating combo:  (2e-07, 2000.0)\n",
      "iteration 0 / 1500: loss 66.797701\n",
      "iteration 100 / 1500: loss 55.316977\n",
      "iteration 200 / 1500: loss 47.124835\n",
      "iteration 300 / 1500: loss 39.975013\n",
      "iteration 400 / 1500: loss 34.185792\n",
      "iteration 500 / 1500: loss 29.291251\n",
      "iteration 600 / 1500: loss 25.172417\n",
      "iteration 700 / 1500: loss 21.709380\n",
      "iteration 800 / 1500: loss 18.692249\n",
      "iteration 900 / 1500: loss 16.197212\n",
      "iteration 1000 / 1500: loss 13.968413\n",
      "iteration 1100 / 1500: loss 12.234859\n",
      "iteration 1200 / 1500: loss 10.539444\n",
      "iteration 1300 / 1500: loss 9.305418\n",
      "iteration 1400 / 1500: loss 8.387692\n",
      "\n",
      "Evaluating combo:  (2e-07, 3000.0)\n",
      "iteration 0 / 1500: loss 97.668194\n",
      "iteration 100 / 1500: loss 75.494591\n",
      "iteration 200 / 1500: loss 59.614050\n",
      "iteration 300 / 1500: loss 46.966044\n",
      "iteration 400 / 1500: loss 37.151913\n",
      "iteration 500 / 1500: loss 29.349848\n",
      "iteration 600 / 1500: loss 23.496564\n",
      "iteration 700 / 1500: loss 18.887085\n",
      "iteration 800 / 1500: loss 15.215744\n",
      "iteration 900 / 1500: loss 12.367756\n",
      "iteration 1000 / 1500: loss 10.032034\n",
      "iteration 1100 / 1500: loss 8.294390\n",
      "iteration 1200 / 1500: loss 7.010450\n",
      "iteration 1300 / 1500: loss 5.919508\n",
      "iteration 1400 / 1500: loss 5.032095\n",
      "\n",
      "Evaluating combo:  (2e-07, 4000.0)\n",
      "iteration 0 / 1500: loss 129.316656\n",
      "iteration 100 / 1500: loss 92.312902\n",
      "iteration 200 / 1500: loss 67.179778\n",
      "iteration 300 / 1500: loss 49.055148\n",
      "iteration 400 / 1500: loss 35.994601\n",
      "iteration 500 / 1500: loss 26.646668\n",
      "iteration 600 / 1500: loss 19.701390\n",
      "iteration 700 / 1500: loss 14.836912\n",
      "iteration 800 / 1500: loss 11.234313\n",
      "iteration 900 / 1500: loss 8.612932\n",
      "iteration 1000 / 1500: loss 6.758644\n",
      "iteration 1100 / 1500: loss 5.402758\n",
      "iteration 1200 / 1500: loss 4.550893\n",
      "iteration 1300 / 1500: loss 3.817481\n",
      "iteration 1400 / 1500: loss 3.372946\n",
      "\n",
      "Evaluating combo:  (2e-07, 5000.0)\n",
      "iteration 0 / 1500: loss 159.385147\n",
      "iteration 100 / 1500: loss 105.554082\n",
      "iteration 200 / 1500: loss 70.867128\n",
      "iteration 300 / 1500: loss 48.204172\n",
      "iteration 400 / 1500: loss 32.675988\n",
      "iteration 500 / 1500: loss 22.482965\n",
      "iteration 600 / 1500: loss 15.727427\n",
      "iteration 700 / 1500: loss 11.102558\n",
      "iteration 800 / 1500: loss 8.120405\n",
      "iteration 900 / 1500: loss 6.078402\n",
      "iteration 1000 / 1500: loss 4.673999\n",
      "iteration 1100 / 1500: loss 3.728240\n",
      "iteration 1200 / 1500: loss 3.069336\n",
      "iteration 1300 / 1500: loss 2.708850\n",
      "iteration 1400 / 1500: loss 2.458711\n",
      "\n",
      "Evaluating combo:  (2e-07, 10000.0)\n",
      "iteration 0 / 1500: loss 311.309829\n",
      "iteration 100 / 1500: loss 139.590996\n",
      "iteration 200 / 1500: loss 63.363073\n",
      "iteration 300 / 1500: loss 29.364321\n",
      "iteration 400 / 1500: loss 14.245084\n",
      "iteration 500 / 1500: loss 7.475857\n",
      "iteration 600 / 1500: loss 4.454329\n",
      "iteration 700 / 1500: loss 3.113150\n",
      "iteration 800 / 1500: loss 2.536848\n",
      "iteration 900 / 1500: loss 2.254665\n",
      "iteration 1000 / 1500: loss 2.032692\n",
      "iteration 1100 / 1500: loss 2.047519\n",
      "iteration 1200 / 1500: loss 1.940643\n",
      "iteration 1300 / 1500: loss 2.023557\n",
      "iteration 1400 / 1500: loss 1.961471\n",
      "\n",
      "Evaluating combo:  (2e-07, 20000.0)\n",
      "iteration 0 / 1500: loss 623.040694\n",
      "iteration 100 / 1500: loss 125.325246\n",
      "iteration 200 / 1500: loss 26.553238\n",
      "iteration 300 / 1500: loss 6.907609\n",
      "iteration 400 / 1500: loss 2.952843\n",
      "iteration 500 / 1500: loss 2.321237\n",
      "iteration 600 / 1500: loss 2.136597\n",
      "iteration 700 / 1500: loss 2.109619\n",
      "iteration 800 / 1500: loss 2.030727\n",
      "iteration 900 / 1500: loss 2.103948\n",
      "iteration 1000 / 1500: loss 2.051228\n",
      "iteration 1100 / 1500: loss 2.070897\n",
      "iteration 1200 / 1500: loss 2.031621\n",
      "iteration 1300 / 1500: loss 2.093064\n",
      "iteration 1400 / 1500: loss 2.024573\n",
      "\n",
      "Evaluating combo:  (2e-07, 25000.0)\n",
      "iteration 0 / 1500: loss 767.094430\n",
      "iteration 100 / 1500: loss 103.965029\n",
      "iteration 200 / 1500: loss 15.599845\n",
      "iteration 300 / 1500: loss 3.921639\n",
      "iteration 400 / 1500: loss 2.313441\n",
      "iteration 500 / 1500: loss 2.133940\n",
      "iteration 600 / 1500: loss 2.098116\n",
      "iteration 700 / 1500: loss 2.050682\n",
      "iteration 800 / 1500: loss 2.103426\n",
      "iteration 900 / 1500: loss 2.091824\n",
      "iteration 1000 / 1500: loss 2.078068\n",
      "iteration 1100 / 1500: loss 2.081434\n",
      "iteration 1200 / 1500: loss 2.098944\n",
      "iteration 1300 / 1500: loss 2.066432\n",
      "iteration 1400 / 1500: loss 2.096895\n",
      "\n",
      "Evaluating combo:  (2e-07, 30000.0)\n",
      "iteration 0 / 1500: loss 925.224944\n",
      "iteration 100 / 1500: loss 84.088445\n",
      "iteration 200 / 1500: loss 9.324224\n",
      "iteration 300 / 1500: loss 2.737483\n",
      "iteration 400 / 1500: loss 2.122167\n",
      "iteration 500 / 1500: loss 2.117143\n",
      "iteration 600 / 1500: loss 2.098058\n",
      "iteration 700 / 1500: loss 2.056402\n",
      "iteration 800 / 1500: loss 2.025522\n",
      "iteration 900 / 1500: loss 2.117405\n",
      "iteration 1000 / 1500: loss 2.131655\n",
      "iteration 1100 / 1500: loss 2.078461\n",
      "iteration 1200 / 1500: loss 2.164605\n",
      "iteration 1300 / 1500: loss 2.080843\n",
      "iteration 1400 / 1500: loss 2.077909\n",
      "\n",
      "Evaluating combo:  (2e-07, 35000.0)\n",
      "iteration 0 / 1500: loss 1093.239001\n",
      "iteration 100 / 1500: loss 66.658697\n",
      "iteration 200 / 1500: loss 5.979346\n",
      "iteration 300 / 1500: loss 2.333079\n",
      "iteration 400 / 1500: loss 2.126863\n",
      "iteration 500 / 1500: loss 2.107533\n",
      "iteration 600 / 1500: loss 2.178915\n",
      "iteration 700 / 1500: loss 2.066821\n",
      "iteration 800 / 1500: loss 2.126245\n",
      "iteration 900 / 1500: loss 2.159968\n",
      "iteration 1000 / 1500: loss 2.109603\n",
      "iteration 1100 / 1500: loss 2.101253\n",
      "iteration 1200 / 1500: loss 2.165851\n",
      "iteration 1300 / 1500: loss 2.075378\n",
      "iteration 1400 / 1500: loss 2.090396\n",
      "\n",
      "Evaluating combo:  (2e-07, 40000.0)\n",
      "iteration 0 / 1500: loss 1235.392789\n",
      "iteration 100 / 1500: loss 50.785735\n",
      "iteration 200 / 1500: loss 3.985841\n",
      "iteration 300 / 1500: loss 2.182816\n",
      "iteration 400 / 1500: loss 2.106895\n",
      "iteration 500 / 1500: loss 2.128832\n",
      "iteration 600 / 1500: loss 2.143496\n",
      "iteration 700 / 1500: loss 2.101455\n",
      "iteration 800 / 1500: loss 2.096585\n",
      "iteration 900 / 1500: loss 2.137719\n",
      "iteration 1000 / 1500: loss 2.155509\n",
      "iteration 1100 / 1500: loss 2.128983\n",
      "iteration 1200 / 1500: loss 2.129042\n",
      "iteration 1300 / 1500: loss 2.156764\n",
      "iteration 1400 / 1500: loss 2.070113\n",
      "\n",
      "Evaluating combo:  (2e-07, 45000.0)\n",
      "iteration 0 / 1500: loss 1390.176296\n",
      "iteration 100 / 1500: loss 38.606239\n",
      "iteration 200 / 1500: loss 3.100366\n",
      "iteration 300 / 1500: loss 2.183634\n",
      "iteration 400 / 1500: loss 2.155846\n",
      "iteration 500 / 1500: loss 2.157001\n",
      "iteration 600 / 1500: loss 2.163693\n",
      "iteration 700 / 1500: loss 2.138396\n",
      "iteration 800 / 1500: loss 2.161792\n",
      "iteration 900 / 1500: loss 2.126757\n",
      "iteration 1000 / 1500: loss 2.109269\n",
      "iteration 1100 / 1500: loss 2.105033\n",
      "iteration 1200 / 1500: loss 2.086265\n",
      "iteration 1300 / 1500: loss 2.121000\n",
      "iteration 1400 / 1500: loss 2.167659\n",
      "\n",
      "Evaluating combo:  (2e-07, 50000.0)\n",
      "iteration 0 / 1500: loss 1561.178827\n",
      "iteration 100 / 1500: loss 29.345431\n",
      "iteration 200 / 1500: loss 2.652343\n",
      "iteration 300 / 1500: loss 2.173910\n",
      "iteration 400 / 1500: loss 2.103159\n",
      "iteration 500 / 1500: loss 2.141857\n",
      "iteration 600 / 1500: loss 2.137575\n",
      "iteration 700 / 1500: loss 2.129677\n",
      "iteration 800 / 1500: loss 2.088165\n",
      "iteration 900 / 1500: loss 2.088921\n",
      "iteration 1000 / 1500: loss 2.134131\n",
      "iteration 1100 / 1500: loss 2.172017\n",
      "iteration 1200 / 1500: loss 2.187471\n",
      "iteration 1300 / 1500: loss 2.109017\n",
      "iteration 1400 / 1500: loss 2.110450\n",
      "\n",
      "Evaluating combo:  (3e-07, 1000.0)\n",
      "iteration 0 / 1500: loss 36.107065\n",
      "iteration 100 / 1500: loss 30.385320\n",
      "iteration 200 / 1500: loss 26.619777\n",
      "iteration 300 / 1500: loss 23.640092\n",
      "iteration 400 / 1500: loss 21.075566\n",
      "iteration 500 / 1500: loss 18.943001\n",
      "iteration 600 / 1500: loss 16.646758\n",
      "iteration 700 / 1500: loss 14.992179\n",
      "iteration 800 / 1500: loss 13.348420\n",
      "iteration 900 / 1500: loss 12.024077\n",
      "iteration 1000 / 1500: loss 10.950978\n",
      "iteration 1100 / 1500: loss 9.752975\n",
      "iteration 1200 / 1500: loss 9.032778\n",
      "iteration 1300 / 1500: loss 8.112644\n",
      "iteration 1400 / 1500: loss 7.353002\n",
      "\n",
      "Evaluating combo:  (3e-07, 2000.0)\n",
      "iteration 0 / 1500: loss 67.866076\n",
      "iteration 100 / 1500: loss 51.635500\n",
      "iteration 200 / 1500: loss 40.653346\n",
      "iteration 300 / 1500: loss 32.155899\n",
      "iteration 400 / 1500: loss 25.404481\n",
      "iteration 500 / 1500: loss 20.481013\n",
      "iteration 600 / 1500: loss 16.265205\n",
      "iteration 700 / 1500: loss 13.239904\n",
      "iteration 800 / 1500: loss 10.751611\n",
      "iteration 900 / 1500: loss 8.841290\n",
      "iteration 1000 / 1500: loss 7.409469\n",
      "iteration 1100 / 1500: loss 6.039697\n",
      "iteration 1200 / 1500: loss 5.204642\n",
      "iteration 1300 / 1500: loss 4.472660\n",
      "iteration 1400 / 1500: loss 3.907571\n",
      "\n",
      "Evaluating combo:  (3e-07, 3000.0)\n",
      "iteration 0 / 1500: loss 97.814803\n",
      "iteration 100 / 1500: loss 66.989614\n",
      "iteration 200 / 1500: loss 47.087124\n",
      "iteration 300 / 1500: loss 33.198230\n",
      "iteration 400 / 1500: loss 23.659919\n",
      "iteration 500 / 1500: loss 17.056903\n",
      "iteration 600 / 1500: loss 12.332266\n",
      "iteration 700 / 1500: loss 9.217139\n",
      "iteration 800 / 1500: loss 6.940735\n",
      "iteration 900 / 1500: loss 5.448762\n",
      "iteration 1000 / 1500: loss 4.302364\n",
      "iteration 1100 / 1500: loss 3.548824\n",
      "iteration 1200 / 1500: loss 3.160143\n",
      "iteration 1300 / 1500: loss 2.886111\n",
      "iteration 1400 / 1500: loss 2.496198\n",
      "\n",
      "Evaluating combo:  (3e-07, 4000.0)\n",
      "iteration 0 / 1500: loss 127.467911\n",
      "iteration 100 / 1500: loss 78.243569\n",
      "iteration 200 / 1500: loss 48.922202\n",
      "iteration 300 / 1500: loss 30.721426\n",
      "iteration 400 / 1500: loss 19.646046\n",
      "iteration 500 / 1500: loss 12.786146\n",
      "iteration 600 / 1500: loss 8.686879\n",
      "iteration 700 / 1500: loss 6.027610\n",
      "iteration 800 / 1500: loss 4.473213\n",
      "iteration 900 / 1500: loss 3.452675\n",
      "iteration 1000 / 1500: loss 2.849953\n",
      "iteration 1100 / 1500: loss 2.553335\n",
      "iteration 1200 / 1500: loss 2.312440\n",
      "iteration 1300 / 1500: loss 2.218740\n",
      "iteration 1400 / 1500: loss 2.078456\n",
      "\n",
      "Evaluating combo:  (3e-07, 5000.0)\n",
      "iteration 0 / 1500: loss 157.789600\n",
      "iteration 100 / 1500: loss 86.009367\n",
      "iteration 200 / 1500: loss 47.791083\n",
      "iteration 300 / 1500: loss 26.900893\n",
      "iteration 400 / 1500: loss 15.542149\n",
      "iteration 500 / 1500: loss 9.394429\n",
      "iteration 600 / 1500: loss 6.009859\n",
      "iteration 700 / 1500: loss 4.172984\n",
      "iteration 800 / 1500: loss 3.089621\n",
      "iteration 900 / 1500: loss 2.728075\n",
      "iteration 1000 / 1500: loss 2.272195\n",
      "iteration 1100 / 1500: loss 2.058535\n",
      "iteration 1200 / 1500: loss 1.962413\n",
      "iteration 1300 / 1500: loss 1.921899\n",
      "iteration 1400 / 1500: loss 1.881674\n",
      "\n",
      "Evaluating combo:  (3e-07, 10000.0)\n",
      "iteration 0 / 1500: loss 312.808280\n",
      "iteration 100 / 1500: loss 94.090007\n",
      "iteration 200 / 1500: loss 29.474620\n",
      "iteration 300 / 1500: loss 10.163500\n",
      "iteration 400 / 1500: loss 4.457172\n",
      "iteration 500 / 1500: loss 2.756080\n",
      "iteration 600 / 1500: loss 2.217932\n",
      "iteration 700 / 1500: loss 2.052267\n",
      "iteration 800 / 1500: loss 2.039291\n",
      "iteration 900 / 1500: loss 1.977033\n",
      "iteration 1000 / 1500: loss 1.907497\n",
      "iteration 1100 / 1500: loss 2.012960\n",
      "iteration 1200 / 1500: loss 2.029127\n",
      "iteration 1300 / 1500: loss 2.005669\n",
      "iteration 1400 / 1500: loss 1.977695\n",
      "\n",
      "Evaluating combo:  (3e-07, 20000.0)\n",
      "iteration 0 / 1500: loss 618.452348\n",
      "iteration 100 / 1500: loss 56.677092\n",
      "iteration 200 / 1500: loss 6.894213\n",
      "iteration 300 / 1500: loss 2.508270\n",
      "iteration 400 / 1500: loss 2.203173\n",
      "iteration 500 / 1500: loss 2.020313\n",
      "iteration 600 / 1500: loss 2.082855\n",
      "iteration 700 / 1500: loss 2.134149\n",
      "iteration 800 / 1500: loss 2.008517\n",
      "iteration 900 / 1500: loss 2.044260\n",
      "iteration 1000 / 1500: loss 2.035897\n",
      "iteration 1100 / 1500: loss 2.118222\n",
      "iteration 1200 / 1500: loss 2.071088\n",
      "iteration 1300 / 1500: loss 2.086140\n",
      "iteration 1400 / 1500: loss 2.088877\n",
      "\n",
      "Evaluating combo:  (3e-07, 25000.0)\n",
      "iteration 0 / 1500: loss 777.618251\n",
      "iteration 100 / 1500: loss 39.438943\n",
      "iteration 200 / 1500: loss 3.908896\n",
      "iteration 300 / 1500: loss 2.127555\n",
      "iteration 400 / 1500: loss 2.110965\n",
      "iteration 500 / 1500: loss 2.079435\n",
      "iteration 600 / 1500: loss 2.106939\n",
      "iteration 700 / 1500: loss 2.045484\n",
      "iteration 800 / 1500: loss 2.080700\n",
      "iteration 900 / 1500: loss 2.005927\n",
      "iteration 1000 / 1500: loss 2.123168\n",
      "iteration 1100 / 1500: loss 2.073698\n",
      "iteration 1200 / 1500: loss 2.079142\n",
      "iteration 1300 / 1500: loss 2.035705\n",
      "iteration 1400 / 1500: loss 2.129740\n",
      "\n",
      "Evaluating combo:  (3e-07, 30000.0)\n",
      "iteration 0 / 1500: loss 930.464442\n",
      "iteration 100 / 1500: loss 26.463998\n",
      "iteration 200 / 1500: loss 2.770193\n",
      "iteration 300 / 1500: loss 2.143556\n",
      "iteration 400 / 1500: loss 2.120295\n",
      "iteration 500 / 1500: loss 2.101755\n",
      "iteration 600 / 1500: loss 2.089971\n",
      "iteration 700 / 1500: loss 2.093633\n",
      "iteration 800 / 1500: loss 2.123113\n",
      "iteration 900 / 1500: loss 2.089401\n",
      "iteration 1000 / 1500: loss 2.073140\n",
      "iteration 1100 / 1500: loss 2.095708\n",
      "iteration 1200 / 1500: loss 2.154769\n",
      "iteration 1300 / 1500: loss 2.039558\n",
      "iteration 1400 / 1500: loss 2.126172\n",
      "\n",
      "Evaluating combo:  (3e-07, 35000.0)\n",
      "iteration 0 / 1500: loss 1069.226528\n",
      "iteration 100 / 1500: loss 17.316202\n",
      "iteration 200 / 1500: loss 2.362552\n",
      "iteration 300 / 1500: loss 2.161013\n",
      "iteration 400 / 1500: loss 2.132652\n",
      "iteration 500 / 1500: loss 2.066745\n",
      "iteration 600 / 1500: loss 2.120907\n",
      "iteration 700 / 1500: loss 2.124844\n",
      "iteration 800 / 1500: loss 2.120202\n",
      "iteration 900 / 1500: loss 2.152927\n",
      "iteration 1000 / 1500: loss 2.105159\n",
      "iteration 1100 / 1500: loss 2.094909\n",
      "iteration 1200 / 1500: loss 2.103211\n",
      "iteration 1300 / 1500: loss 2.093126\n",
      "iteration 1400 / 1500: loss 2.131407\n",
      "\n",
      "Evaluating combo:  (3e-07, 40000.0)\n",
      "iteration 0 / 1500: loss 1239.596118\n",
      "iteration 100 / 1500: loss 11.688949\n",
      "iteration 200 / 1500: loss 2.188650\n",
      "iteration 300 / 1500: loss 2.137545\n",
      "iteration 400 / 1500: loss 2.126096\n",
      "iteration 500 / 1500: loss 2.118781\n",
      "iteration 600 / 1500: loss 2.131313\n",
      "iteration 700 / 1500: loss 2.108938\n",
      "iteration 800 / 1500: loss 2.119981\n",
      "iteration 900 / 1500: loss 2.152490\n",
      "iteration 1000 / 1500: loss 2.125174\n",
      "iteration 1100 / 1500: loss 2.144974\n",
      "iteration 1200 / 1500: loss 2.078424\n",
      "iteration 1300 / 1500: loss 2.131492\n",
      "iteration 1400 / 1500: loss 2.170639\n",
      "\n",
      "Evaluating combo:  (3e-07, 45000.0)\n",
      "iteration 0 / 1500: loss 1395.537576\n",
      "iteration 100 / 1500: loss 7.935335\n",
      "iteration 200 / 1500: loss 2.206975\n",
      "iteration 300 / 1500: loss 2.101840\n",
      "iteration 400 / 1500: loss 2.121086\n",
      "iteration 500 / 1500: loss 2.121962\n",
      "iteration 600 / 1500: loss 2.098773\n",
      "iteration 700 / 1500: loss 2.129112\n",
      "iteration 800 / 1500: loss 2.137863\n",
      "iteration 900 / 1500: loss 2.084911\n",
      "iteration 1000 / 1500: loss 2.149121\n",
      "iteration 1100 / 1500: loss 2.158036\n",
      "iteration 1200 / 1500: loss 2.156364\n",
      "iteration 1300 / 1500: loss 2.143417\n",
      "iteration 1400 / 1500: loss 2.132940\n",
      "\n",
      "Evaluating combo:  (3e-07, 50000.0)\n",
      "iteration 0 / 1500: loss 1570.758755\n",
      "iteration 100 / 1500: loss 5.663140\n",
      "iteration 200 / 1500: loss 2.114971\n",
      "iteration 300 / 1500: loss 2.189467\n",
      "iteration 400 / 1500: loss 2.123477\n",
      "iteration 500 / 1500: loss 2.164918\n",
      "iteration 600 / 1500: loss 2.150192\n",
      "iteration 700 / 1500: loss 2.107128\n",
      "iteration 800 / 1500: loss 2.105080\n",
      "iteration 900 / 1500: loss 2.133747\n",
      "iteration 1000 / 1500: loss 2.114655\n",
      "iteration 1100 / 1500: loss 2.138007\n",
      "iteration 1200 / 1500: loss 2.169960\n",
      "iteration 1300 / 1500: loss 2.165672\n",
      "iteration 1400 / 1500: loss 2.161223\n",
      "\n",
      "Evaluating combo:  (4e-07, 1000.0)\n",
      "iteration 0 / 1500: loss 37.901139\n",
      "iteration 100 / 1500: loss 29.018218\n",
      "iteration 200 / 1500: loss 24.473105\n",
      "iteration 300 / 1500: loss 21.245244\n",
      "iteration 400 / 1500: loss 18.000691\n",
      "iteration 500 / 1500: loss 15.810765\n",
      "iteration 600 / 1500: loss 13.478815\n",
      "iteration 700 / 1500: loss 11.743905\n",
      "iteration 800 / 1500: loss 10.189647\n",
      "iteration 900 / 1500: loss 8.998392\n",
      "iteration 1000 / 1500: loss 7.908532\n",
      "iteration 1100 / 1500: loss 6.872584\n",
      "iteration 1200 / 1500: loss 6.081007\n",
      "iteration 1300 / 1500: loss 5.434214\n",
      "iteration 1400 / 1500: loss 4.863250\n",
      "\n",
      "Evaluating combo:  (4e-07, 2000.0)\n",
      "iteration 0 / 1500: loss 67.563195\n",
      "iteration 100 / 1500: loss 47.277435\n",
      "iteration 200 / 1500: loss 34.549540\n",
      "iteration 300 / 1500: loss 25.405818\n",
      "iteration 400 / 1500: loss 18.780784\n",
      "iteration 500 / 1500: loss 14.140343\n",
      "iteration 600 / 1500: loss 10.640693\n",
      "iteration 700 / 1500: loss 8.398279\n",
      "iteration 800 / 1500: loss 6.580202\n",
      "iteration 900 / 1500: loss 5.263221\n",
      "iteration 1000 / 1500: loss 4.326340\n",
      "iteration 1100 / 1500: loss 3.699037\n",
      "iteration 1200 / 1500: loss 3.108946\n",
      "iteration 1300 / 1500: loss 2.712284\n",
      "iteration 1400 / 1500: loss 2.447337\n",
      "\n",
      "Evaluating combo:  (4e-07, 3000.0)\n",
      "iteration 0 / 1500: loss 97.880265\n",
      "iteration 100 / 1500: loss 59.425013\n",
      "iteration 200 / 1500: loss 36.961921\n",
      "iteration 300 / 1500: loss 23.441944\n",
      "iteration 400 / 1500: loss 15.305854\n",
      "iteration 500 / 1500: loss 10.089759\n",
      "iteration 600 / 1500: loss 6.947354\n",
      "iteration 700 / 1500: loss 4.942109\n",
      "iteration 800 / 1500: loss 3.775664\n",
      "iteration 900 / 1500: loss 3.083304\n",
      "iteration 1000 / 1500: loss 2.718879\n",
      "iteration 1100 / 1500: loss 2.388233\n",
      "iteration 1200 / 1500: loss 2.156805\n",
      "iteration 1300 / 1500: loss 2.040697\n",
      "iteration 1400 / 1500: loss 1.942659\n",
      "\n",
      "Evaluating combo:  (4e-07, 4000.0)\n",
      "iteration 0 / 1500: loss 127.783988\n",
      "iteration 100 / 1500: loss 66.664291\n",
      "iteration 200 / 1500: loss 35.725907\n",
      "iteration 300 / 1500: loss 19.492010\n",
      "iteration 400 / 1500: loss 11.239485\n",
      "iteration 500 / 1500: loss 6.719586\n",
      "iteration 600 / 1500: loss 4.557456\n",
      "iteration 700 / 1500: loss 3.267461\n",
      "iteration 800 / 1500: loss 2.589335\n",
      "iteration 900 / 1500: loss 2.254433\n",
      "iteration 1000 / 1500: loss 2.177708\n",
      "iteration 1100 / 1500: loss 1.939742\n",
      "iteration 1200 / 1500: loss 1.958772\n",
      "iteration 1300 / 1500: loss 1.939727\n",
      "iteration 1400 / 1500: loss 1.894179\n",
      "\n",
      "Evaluating combo:  (4e-07, 5000.0)\n",
      "iteration 0 / 1500: loss 159.438538\n",
      "iteration 100 / 1500: loss 70.549142\n",
      "iteration 200 / 1500: loss 32.465194\n",
      "iteration 300 / 1500: loss 15.546893\n",
      "iteration 400 / 1500: loss 8.083417\n",
      "iteration 500 / 1500: loss 4.657769\n",
      "iteration 600 / 1500: loss 3.137484\n",
      "iteration 700 / 1500: loss 2.379504\n",
      "iteration 800 / 1500: loss 2.121523\n",
      "iteration 900 / 1500: loss 2.034054\n",
      "iteration 1000 / 1500: loss 2.061053\n",
      "iteration 1100 / 1500: loss 1.941191\n",
      "iteration 1200 / 1500: loss 1.905392\n",
      "iteration 1300 / 1500: loss 1.945373\n",
      "iteration 1400 / 1500: loss 1.920935\n",
      "\n",
      "Evaluating combo:  (4e-07, 10000.0)\n",
      "iteration 0 / 1500: loss 316.998496\n",
      "iteration 100 / 1500: loss 64.025425\n",
      "iteration 200 / 1500: loss 14.424530\n",
      "iteration 300 / 1500: loss 4.403833\n",
      "iteration 400 / 1500: loss 2.446246\n",
      "iteration 500 / 1500: loss 2.142116\n",
      "iteration 600 / 1500: loss 2.017065\n",
      "iteration 700 / 1500: loss 1.949985\n",
      "iteration 800 / 1500: loss 2.008986\n",
      "iteration 900 / 1500: loss 2.036100\n",
      "iteration 1000 / 1500: loss 2.031921\n",
      "iteration 1100 / 1500: loss 2.004223\n",
      "iteration 1200 / 1500: loss 1.996458\n",
      "iteration 1300 / 1500: loss 1.990379\n",
      "iteration 1400 / 1500: loss 2.026633\n",
      "\n",
      "Evaluating combo:  (4e-07, 20000.0)\n",
      "iteration 0 / 1500: loss 620.330551\n",
      "iteration 100 / 1500: loss 26.406387\n",
      "iteration 200 / 1500: loss 2.998227\n",
      "iteration 300 / 1500: loss 2.138478\n",
      "iteration 400 / 1500: loss 2.088136\n",
      "iteration 500 / 1500: loss 2.074935\n",
      "iteration 600 / 1500: loss 2.038535\n",
      "iteration 700 / 1500: loss 2.018937\n",
      "iteration 800 / 1500: loss 2.045445\n",
      "iteration 900 / 1500: loss 2.010887\n",
      "iteration 1000 / 1500: loss 2.073526\n",
      "iteration 1100 / 1500: loss 2.059623\n",
      "iteration 1200 / 1500: loss 2.118447\n",
      "iteration 1300 / 1500: loss 2.067245\n",
      "iteration 1400 / 1500: loss 2.077298\n",
      "\n",
      "Evaluating combo:  (4e-07, 25000.0)\n",
      "iteration 0 / 1500: loss 769.376328\n",
      "iteration 100 / 1500: loss 15.426213\n",
      "iteration 200 / 1500: loss 2.301549\n",
      "iteration 300 / 1500: loss 2.033298\n",
      "iteration 400 / 1500: loss 2.064174\n",
      "iteration 500 / 1500: loss 2.073211\n",
      "iteration 600 / 1500: loss 2.047927\n",
      "iteration 700 / 1500: loss 2.142534\n",
      "iteration 800 / 1500: loss 2.011213\n",
      "iteration 900 / 1500: loss 2.162516\n",
      "iteration 1000 / 1500: loss 2.132955\n",
      "iteration 1100 / 1500: loss 2.074623\n",
      "iteration 1200 / 1500: loss 2.086606\n",
      "iteration 1300 / 1500: loss 2.089391\n",
      "iteration 1400 / 1500: loss 2.121898\n",
      "\n",
      "Evaluating combo:  (4e-07, 30000.0)\n",
      "iteration 0 / 1500: loss 932.749576\n",
      "iteration 100 / 1500: loss 9.201898\n",
      "iteration 200 / 1500: loss 2.203275\n",
      "iteration 300 / 1500: loss 2.144407\n",
      "iteration 400 / 1500: loss 2.085931\n",
      "iteration 500 / 1500: loss 2.182116\n",
      "iteration 600 / 1500: loss 2.070577\n",
      "iteration 700 / 1500: loss 2.049614\n",
      "iteration 800 / 1500: loss 2.164518\n",
      "iteration 900 / 1500: loss 2.053225\n",
      "iteration 1000 / 1500: loss 2.077575\n",
      "iteration 1100 / 1500: loss 2.109051\n",
      "iteration 1200 / 1500: loss 2.050944\n",
      "iteration 1300 / 1500: loss 2.076085\n",
      "iteration 1400 / 1500: loss 2.101725\n",
      "\n",
      "Evaluating combo:  (4e-07, 35000.0)\n",
      "iteration 0 / 1500: loss 1077.297856\n",
      "iteration 100 / 1500: loss 5.751760\n",
      "iteration 200 / 1500: loss 2.180197\n",
      "iteration 300 / 1500: loss 2.089203\n",
      "iteration 400 / 1500: loss 2.137123\n",
      "iteration 500 / 1500: loss 2.126017\n",
      "iteration 600 / 1500: loss 2.105697\n",
      "iteration 700 / 1500: loss 2.149506\n",
      "iteration 800 / 1500: loss 2.162356\n",
      "iteration 900 / 1500: loss 2.103125\n",
      "iteration 1000 / 1500: loss 2.082429\n",
      "iteration 1100 / 1500: loss 2.147513\n",
      "iteration 1200 / 1500: loss 2.146509\n",
      "iteration 1300 / 1500: loss 2.088557\n",
      "iteration 1400 / 1500: loss 2.095536\n",
      "\n",
      "Evaluating combo:  (4e-07, 40000.0)\n",
      "iteration 0 / 1500: loss 1233.659150\n",
      "iteration 100 / 1500: loss 3.931641\n",
      "iteration 200 / 1500: loss 2.128375\n",
      "iteration 300 / 1500: loss 2.132458\n",
      "iteration 400 / 1500: loss 2.104044\n",
      "iteration 500 / 1500: loss 2.132754\n",
      "iteration 600 / 1500: loss 2.108608\n",
      "iteration 700 / 1500: loss 2.152497\n",
      "iteration 800 / 1500: loss 2.159600\n",
      "iteration 900 / 1500: loss 2.117704\n",
      "iteration 1000 / 1500: loss 2.177571\n",
      "iteration 1100 / 1500: loss 2.152074\n",
      "iteration 1200 / 1500: loss 2.123894\n",
      "iteration 1300 / 1500: loss 2.157718\n",
      "iteration 1400 / 1500: loss 2.104794\n",
      "\n",
      "Evaluating combo:  (4e-07, 45000.0)\n",
      "iteration 0 / 1500: loss 1390.197748\n",
      "iteration 100 / 1500: loss 3.038730\n",
      "iteration 200 / 1500: loss 2.140663\n",
      "iteration 300 / 1500: loss 2.161183\n",
      "iteration 400 / 1500: loss 2.073511\n",
      "iteration 500 / 1500: loss 2.130914\n",
      "iteration 600 / 1500: loss 2.163156\n",
      "iteration 700 / 1500: loss 2.154188\n",
      "iteration 800 / 1500: loss 2.131274\n",
      "iteration 900 / 1500: loss 2.177560\n",
      "iteration 1000 / 1500: loss 2.099473\n",
      "iteration 1100 / 1500: loss 2.182183\n",
      "iteration 1200 / 1500: loss 2.115095\n",
      "iteration 1300 / 1500: loss 2.132536\n",
      "iteration 1400 / 1500: loss 2.148549\n",
      "\n",
      "Evaluating combo:  (4e-07, 50000.0)\n",
      "iteration 0 / 1500: loss 1536.702094\n",
      "iteration 100 / 1500: loss 2.600637\n",
      "iteration 200 / 1500: loss 2.084282\n",
      "iteration 300 / 1500: loss 2.169080\n",
      "iteration 400 / 1500: loss 2.171291\n",
      "iteration 500 / 1500: loss 2.174169\n",
      "iteration 600 / 1500: loss 2.169573\n",
      "iteration 700 / 1500: loss 2.162520\n",
      "iteration 800 / 1500: loss 2.119736\n",
      "iteration 900 / 1500: loss 2.147210\n",
      "iteration 1000 / 1500: loss 2.124730\n",
      "iteration 1100 / 1500: loss 2.150878\n",
      "iteration 1200 / 1500: loss 2.152229\n",
      "iteration 1300 / 1500: loss 2.155737\n",
      "iteration 1400 / 1500: loss 2.177253\n",
      "\n",
      "Evaluating combo:  (5e-07, 1000.0)\n",
      "iteration 0 / 1500: loss 35.812806\n",
      "iteration 100 / 1500: loss 27.982445\n",
      "iteration 200 / 1500: loss 22.750247\n",
      "iteration 300 / 1500: loss 18.936731\n",
      "iteration 400 / 1500: loss 15.480072\n",
      "iteration 500 / 1500: loss 13.049154\n",
      "iteration 600 / 1500: loss 11.077889\n",
      "iteration 700 / 1500: loss 9.363632\n",
      "iteration 800 / 1500: loss 7.964954\n",
      "iteration 900 / 1500: loss 6.877298\n",
      "iteration 1000 / 1500: loss 5.791663\n",
      "iteration 1100 / 1500: loss 5.131131\n",
      "iteration 1200 / 1500: loss 4.462001\n",
      "iteration 1300 / 1500: loss 3.938407\n",
      "iteration 1400 / 1500: loss 3.593264\n",
      "\n",
      "Evaluating combo:  (5e-07, 2000.0)\n",
      "iteration 0 / 1500: loss 65.846721\n",
      "iteration 100 / 1500: loss 43.406792\n",
      "iteration 200 / 1500: loss 29.361783\n",
      "iteration 300 / 1500: loss 20.120144\n",
      "iteration 400 / 1500: loss 14.020775\n",
      "iteration 500 / 1500: loss 10.045882\n",
      "iteration 600 / 1500: loss 7.241932\n",
      "iteration 700 / 1500: loss 5.414166\n",
      "iteration 800 / 1500: loss 4.204374\n",
      "iteration 900 / 1500: loss 3.488201\n",
      "iteration 1000 / 1500: loss 2.995968\n",
      "iteration 1100 / 1500: loss 2.512178\n",
      "iteration 1200 / 1500: loss 2.286955\n",
      "iteration 1300 / 1500: loss 2.199414\n",
      "iteration 1400 / 1500: loss 2.132519\n",
      "\n",
      "Evaluating combo:  (5e-07, 3000.0)\n",
      "iteration 0 / 1500: loss 97.668053\n",
      "iteration 100 / 1500: loss 52.928414\n",
      "iteration 200 / 1500: loss 29.476134\n",
      "iteration 300 / 1500: loss 16.910855\n",
      "iteration 400 / 1500: loss 10.049185\n",
      "iteration 500 / 1500: loss 6.297216\n",
      "iteration 600 / 1500: loss 4.367817\n",
      "iteration 700 / 1500: loss 3.284655\n",
      "iteration 800 / 1500: loss 2.693998\n",
      "iteration 900 / 1500: loss 2.327435\n",
      "iteration 1000 / 1500: loss 2.158087\n",
      "iteration 1100 / 1500: loss 2.071602\n",
      "iteration 1200 / 1500: loss 1.829710\n",
      "iteration 1300 / 1500: loss 1.889566\n",
      "iteration 1400 / 1500: loss 1.918073\n",
      "\n",
      "Evaluating combo:  (5e-07, 4000.0)\n",
      "iteration 0 / 1500: loss 128.510184\n",
      "iteration 100 / 1500: loss 57.083343\n",
      "iteration 200 / 1500: loss 26.476146\n",
      "iteration 300 / 1500: loss 12.792577\n",
      "iteration 400 / 1500: loss 6.766696\n",
      "iteration 500 / 1500: loss 4.105780\n",
      "iteration 600 / 1500: loss 2.929949\n",
      "iteration 700 / 1500: loss 2.332931\n",
      "iteration 800 / 1500: loss 2.133489\n",
      "iteration 900 / 1500: loss 1.982730\n",
      "iteration 1000 / 1500: loss 2.048575\n",
      "iteration 1100 / 1500: loss 1.960727\n",
      "iteration 1200 / 1500: loss 1.916917\n",
      "iteration 1300 / 1500: loss 1.977501\n",
      "iteration 1400 / 1500: loss 1.936293\n",
      "\n",
      "Evaluating combo:  (5e-07, 5000.0)\n",
      "iteration 0 / 1500: loss 159.806650\n",
      "iteration 100 / 1500: loss 58.603511\n",
      "iteration 200 / 1500: loss 22.486029\n",
      "iteration 300 / 1500: loss 9.460944\n",
      "iteration 400 / 1500: loss 4.795444\n",
      "iteration 500 / 1500: loss 2.985690\n",
      "iteration 600 / 1500: loss 2.310516\n",
      "iteration 700 / 1500: loss 2.124851\n",
      "iteration 800 / 1500: loss 1.949118\n",
      "iteration 900 / 1500: loss 1.964730\n",
      "iteration 1000 / 1500: loss 1.930750\n",
      "iteration 1100 / 1500: loss 2.017967\n",
      "iteration 1200 / 1500: loss 2.010590\n",
      "iteration 1300 / 1500: loss 1.909028\n",
      "iteration 1400 / 1500: loss 1.920977\n",
      "\n",
      "Evaluating combo:  (5e-07, 10000.0)\n",
      "iteration 0 / 1500: loss 314.066623\n",
      "iteration 100 / 1500: loss 43.101373\n",
      "iteration 200 / 1500: loss 7.435490\n",
      "iteration 300 / 1500: loss 2.721699\n",
      "iteration 400 / 1500: loss 2.136395\n",
      "iteration 500 / 1500: loss 1.985357\n",
      "iteration 600 / 1500: loss 2.054981\n",
      "iteration 700 / 1500: loss 2.036781\n",
      "iteration 800 / 1500: loss 1.975307\n",
      "iteration 900 / 1500: loss 1.967404\n",
      "iteration 1000 / 1500: loss 2.023613\n",
      "iteration 1100 / 1500: loss 1.984206\n",
      "iteration 1200 / 1500: loss 1.965379\n",
      "iteration 1300 / 1500: loss 2.031250\n",
      "iteration 1400 / 1500: loss 2.019962\n",
      "\n",
      "Evaluating combo:  (5e-07, 20000.0)\n",
      "iteration 0 / 1500: loss 615.269602\n",
      "iteration 100 / 1500: loss 12.695752\n",
      "iteration 200 / 1500: loss 2.376098\n",
      "iteration 300 / 1500: loss 2.097099\n",
      "iteration 400 / 1500: loss 2.094188\n",
      "iteration 500 / 1500: loss 2.100300\n",
      "iteration 600 / 1500: loss 2.051544\n",
      "iteration 700 / 1500: loss 2.067731\n",
      "iteration 800 / 1500: loss 2.054267\n",
      "iteration 900 / 1500: loss 2.112667\n",
      "iteration 1000 / 1500: loss 2.048724\n",
      "iteration 1100 / 1500: loss 2.032122\n",
      "iteration 1200 / 1500: loss 2.094051\n",
      "iteration 1300 / 1500: loss 2.060319\n",
      "iteration 1400 / 1500: loss 2.037031\n",
      "\n",
      "Evaluating combo:  (5e-07, 25000.0)\n",
      "iteration 0 / 1500: loss 767.195339\n",
      "iteration 100 / 1500: loss 6.902046\n",
      "iteration 200 / 1500: loss 2.143481\n",
      "iteration 300 / 1500: loss 2.146577\n",
      "iteration 400 / 1500: loss 2.102156\n",
      "iteration 500 / 1500: loss 2.100977\n",
      "iteration 600 / 1500: loss 2.053984\n",
      "iteration 700 / 1500: loss 2.125295\n",
      "iteration 800 / 1500: loss 2.064659\n",
      "iteration 900 / 1500: loss 2.100293\n",
      "iteration 1000 / 1500: loss 2.070593\n",
      "iteration 1100 / 1500: loss 2.060467\n",
      "iteration 1200 / 1500: loss 2.069897\n",
      "iteration 1300 / 1500: loss 2.079476\n",
      "iteration 1400 / 1500: loss 2.135460\n",
      "\n",
      "Evaluating combo:  (5e-07, 30000.0)\n",
      "iteration 0 / 1500: loss 945.127392\n",
      "iteration 100 / 1500: loss 4.199351\n",
      "iteration 200 / 1500: loss 2.124305\n",
      "iteration 300 / 1500: loss 2.181051\n",
      "iteration 400 / 1500: loss 2.132549\n",
      "iteration 500 / 1500: loss 2.097367\n",
      "iteration 600 / 1500: loss 2.122798\n",
      "iteration 700 / 1500: loss 2.026431\n",
      "iteration 800 / 1500: loss 2.104194\n",
      "iteration 900 / 1500: loss 2.093115\n",
      "iteration 1000 / 1500: loss 2.122161\n",
      "iteration 1100 / 1500: loss 2.101959\n",
      "iteration 1200 / 1500: loss 2.108391\n",
      "iteration 1300 / 1500: loss 2.074842\n",
      "iteration 1400 / 1500: loss 2.173793\n",
      "\n",
      "Evaluating combo:  (5e-07, 35000.0)\n",
      "iteration 0 / 1500: loss 1080.424278\n",
      "iteration 100 / 1500: loss 2.984940\n",
      "iteration 200 / 1500: loss 2.164951\n",
      "iteration 300 / 1500: loss 2.122537\n",
      "iteration 400 / 1500: loss 2.075229\n",
      "iteration 500 / 1500: loss 2.089215\n",
      "iteration 600 / 1500: loss 2.109392\n",
      "iteration 700 / 1500: loss 2.151918\n",
      "iteration 800 / 1500: loss 2.113767\n",
      "iteration 900 / 1500: loss 2.120211\n",
      "iteration 1000 / 1500: loss 2.099884\n",
      "iteration 1100 / 1500: loss 2.120098\n",
      "iteration 1200 / 1500: loss 2.052176\n",
      "iteration 1300 / 1500: loss 2.088149\n",
      "iteration 1400 / 1500: loss 2.119646\n",
      "\n",
      "Evaluating combo:  (5e-07, 40000.0)\n",
      "iteration 0 / 1500: loss 1243.385409\n",
      "iteration 100 / 1500: loss 2.456966\n",
      "iteration 200 / 1500: loss 2.120506\n",
      "iteration 300 / 1500: loss 2.162144\n",
      "iteration 400 / 1500: loss 2.098804\n",
      "iteration 500 / 1500: loss 2.191055\n",
      "iteration 600 / 1500: loss 2.094119\n",
      "iteration 700 / 1500: loss 2.111799\n",
      "iteration 800 / 1500: loss 2.154194\n",
      "iteration 900 / 1500: loss 2.183234\n",
      "iteration 1000 / 1500: loss 2.098831\n",
      "iteration 1100 / 1500: loss 2.153681\n",
      "iteration 1200 / 1500: loss 2.157913\n",
      "iteration 1300 / 1500: loss 2.098714\n",
      "iteration 1400 / 1500: loss 2.132951\n",
      "\n",
      "Evaluating combo:  (5e-07, 45000.0)\n",
      "iteration 0 / 1500: loss 1372.168885\n",
      "iteration 100 / 1500: loss 2.262175\n",
      "iteration 200 / 1500: loss 2.148545\n",
      "iteration 300 / 1500: loss 2.118737\n",
      "iteration 400 / 1500: loss 2.112228\n",
      "iteration 500 / 1500: loss 2.092116\n",
      "iteration 600 / 1500: loss 2.136431\n",
      "iteration 700 / 1500: loss 2.102170\n",
      "iteration 800 / 1500: loss 2.130896\n",
      "iteration 900 / 1500: loss 2.142475\n",
      "iteration 1000 / 1500: loss 2.155248\n",
      "iteration 1100 / 1500: loss 2.138252\n",
      "iteration 1200 / 1500: loss 2.113974\n",
      "iteration 1300 / 1500: loss 2.143465\n",
      "iteration 1400 / 1500: loss 2.147658\n",
      "\n",
      "Evaluating combo:  (5e-07, 50000.0)\n",
      "iteration 0 / 1500: loss 1550.779058\n",
      "iteration 100 / 1500: loss 2.229489\n",
      "iteration 200 / 1500: loss 2.119211\n",
      "iteration 300 / 1500: loss 2.108599\n",
      "iteration 400 / 1500: loss 2.158733\n",
      "iteration 500 / 1500: loss 2.118206\n",
      "iteration 600 / 1500: loss 2.152168\n",
      "iteration 700 / 1500: loss 2.140372\n",
      "iteration 800 / 1500: loss 2.187110\n",
      "iteration 900 / 1500: loss 2.147585\n",
      "iteration 1000 / 1500: loss 2.181513\n",
      "iteration 1100 / 1500: loss 2.142064\n",
      "iteration 1200 / 1500: loss 2.135275\n",
      "iteration 1300 / 1500: loss 2.159777\n",
      "iteration 1400 / 1500: loss 2.123660\n",
      "\n",
      "Evaluating combo:  (1e-06, 1000.0)\n",
      "iteration 0 / 1500: loss 36.057838\n",
      "iteration 100 / 1500: loss 22.803303\n",
      "iteration 200 / 1500: loss 15.620620\n",
      "iteration 300 / 1500: loss 10.866029\n",
      "iteration 400 / 1500: loss 7.844211\n",
      "iteration 500 / 1500: loss 5.830658\n",
      "iteration 600 / 1500: loss 4.484778\n",
      "iteration 700 / 1500: loss 3.553141\n",
      "iteration 800 / 1500: loss 3.028683\n",
      "iteration 900 / 1500: loss 2.671083\n",
      "iteration 1000 / 1500: loss 2.342638\n",
      "iteration 1100 / 1500: loss 2.210332\n",
      "iteration 1200 / 1500: loss 2.027959\n",
      "iteration 1300 / 1500: loss 1.989580\n",
      "iteration 1400 / 1500: loss 1.870431\n",
      "\n",
      "Evaluating combo:  (1e-06, 2000.0)\n",
      "iteration 0 / 1500: loss 66.776012\n",
      "iteration 100 / 1500: loss 29.502672\n",
      "iteration 200 / 1500: loss 13.866010\n",
      "iteration 300 / 1500: loss 7.245334\n",
      "iteration 400 / 1500: loss 4.277548\n",
      "iteration 500 / 1500: loss 3.045559\n",
      "iteration 600 / 1500: loss 2.386364\n",
      "iteration 700 / 1500: loss 2.021337\n",
      "iteration 800 / 1500: loss 1.927749\n",
      "iteration 900 / 1500: loss 1.957060\n",
      "iteration 1000 / 1500: loss 1.901343\n",
      "iteration 1100 / 1500: loss 1.823686\n",
      "iteration 1200 / 1500: loss 1.883721\n",
      "iteration 1300 / 1500: loss 1.861582\n",
      "iteration 1400 / 1500: loss 1.783748\n",
      "\n",
      "Evaluating combo:  (1e-06, 3000.0)\n",
      "iteration 0 / 1500: loss 98.108771\n",
      "iteration 100 / 1500: loss 29.840753\n",
      "iteration 200 / 1500: loss 10.155077\n",
      "iteration 300 / 1500: loss 4.355005\n",
      "iteration 400 / 1500: loss 2.652988\n",
      "iteration 500 / 1500: loss 2.081698\n",
      "iteration 600 / 1500: loss 1.972891\n",
      "iteration 700 / 1500: loss 1.824112\n",
      "iteration 800 / 1500: loss 1.786844\n",
      "iteration 900 / 1500: loss 1.873728\n",
      "iteration 1000 / 1500: loss 1.923449\n",
      "iteration 1100 / 1500: loss 1.940924\n",
      "iteration 1200 / 1500: loss 1.887562\n",
      "iteration 1300 / 1500: loss 1.892557\n",
      "iteration 1400 / 1500: loss 1.813943\n",
      "\n",
      "Evaluating combo:  (1e-06, 4000.0)\n",
      "iteration 0 / 1500: loss 127.965442\n",
      "iteration 100 / 1500: loss 26.343823\n",
      "iteration 200 / 1500: loss 6.722108\n",
      "iteration 300 / 1500: loss 2.908616\n",
      "iteration 400 / 1500: loss 2.077245\n",
      "iteration 500 / 1500: loss 2.028608\n",
      "iteration 600 / 1500: loss 1.935313\n",
      "iteration 700 / 1500: loss 1.914540\n",
      "iteration 800 / 1500: loss 1.888892\n",
      "iteration 900 / 1500: loss 1.935659\n",
      "iteration 1000 / 1500: loss 1.983442\n",
      "iteration 1100 / 1500: loss 1.871282\n",
      "iteration 1200 / 1500: loss 1.938469\n",
      "iteration 1300 / 1500: loss 1.984764\n",
      "iteration 1400 / 1500: loss 1.935268\n",
      "\n",
      "Evaluating combo:  (1e-06, 5000.0)\n",
      "iteration 0 / 1500: loss 158.773838\n",
      "iteration 100 / 1500: loss 22.203043\n",
      "iteration 200 / 1500: loss 4.628663\n",
      "iteration 300 / 1500: loss 2.311167\n",
      "iteration 400 / 1500: loss 1.946683\n",
      "iteration 500 / 1500: loss 1.933134\n",
      "iteration 600 / 1500: loss 1.994460\n",
      "iteration 700 / 1500: loss 1.992478\n",
      "iteration 800 / 1500: loss 1.839751\n",
      "iteration 900 / 1500: loss 1.962057\n",
      "iteration 1000 / 1500: loss 1.946635\n",
      "iteration 1100 / 1500: loss 1.912137\n",
      "iteration 1200 / 1500: loss 1.843789\n",
      "iteration 1300 / 1500: loss 1.998462\n",
      "iteration 1400 / 1500: loss 1.984484\n",
      "\n",
      "Evaluating combo:  (1e-06, 10000.0)\n",
      "iteration 0 / 1500: loss 310.335758\n",
      "iteration 100 / 1500: loss 7.238834\n",
      "iteration 200 / 1500: loss 2.055359\n",
      "iteration 300 / 1500: loss 2.061451\n",
      "iteration 400 / 1500: loss 1.966205\n",
      "iteration 500 / 1500: loss 2.013780\n",
      "iteration 600 / 1500: loss 2.020039\n",
      "iteration 700 / 1500: loss 2.075212\n",
      "iteration 800 / 1500: loss 1.962213\n",
      "iteration 900 / 1500: loss 2.010326\n",
      "iteration 1000 / 1500: loss 2.018938\n",
      "iteration 1100 / 1500: loss 2.046024\n",
      "iteration 1200 / 1500: loss 1.981316\n",
      "iteration 1300 / 1500: loss 1.987375\n",
      "iteration 1400 / 1500: loss 1.933928\n",
      "\n",
      "Evaluating combo:  (1e-06, 20000.0)\n",
      "iteration 0 / 1500: loss 616.783449\n",
      "iteration 100 / 1500: loss 2.294339\n",
      "iteration 200 / 1500: loss 2.080314\n",
      "iteration 300 / 1500: loss 2.061035\n",
      "iteration 400 / 1500: loss 2.134028\n",
      "iteration 500 / 1500: loss 2.043800\n",
      "iteration 600 / 1500: loss 2.089495\n",
      "iteration 700 / 1500: loss 2.048181\n",
      "iteration 800 / 1500: loss 2.107786\n",
      "iteration 900 / 1500: loss 2.121186\n",
      "iteration 1000 / 1500: loss 2.094258\n",
      "iteration 1100 / 1500: loss 2.074621\n",
      "iteration 1200 / 1500: loss 2.073567\n",
      "iteration 1300 / 1500: loss 2.046989\n",
      "iteration 1400 / 1500: loss 2.048716\n",
      "\n",
      "Evaluating combo:  (1e-06, 25000.0)\n",
      "iteration 0 / 1500: loss 778.874477\n",
      "iteration 100 / 1500: loss 2.046002\n",
      "iteration 200 / 1500: loss 2.086147\n",
      "iteration 300 / 1500: loss 2.118709\n",
      "iteration 400 / 1500: loss 2.131125\n",
      "iteration 500 / 1500: loss 2.075850\n",
      "iteration 600 / 1500: loss 2.099577\n",
      "iteration 700 / 1500: loss 2.107028\n",
      "iteration 800 / 1500: loss 2.071044\n",
      "iteration 900 / 1500: loss 2.041196\n",
      "iteration 1000 / 1500: loss 2.083000\n",
      "iteration 1100 / 1500: loss 2.066104\n",
      "iteration 1200 / 1500: loss 2.130369\n",
      "iteration 1300 / 1500: loss 2.056612\n",
      "iteration 1400 / 1500: loss 2.083797\n",
      "\n",
      "Evaluating combo:  (1e-06, 30000.0)\n",
      "iteration 0 / 1500: loss 923.100298\n",
      "iteration 100 / 1500: loss 2.115807\n",
      "iteration 200 / 1500: loss 2.129481\n",
      "iteration 300 / 1500: loss 2.130318\n",
      "iteration 400 / 1500: loss 2.109622\n",
      "iteration 500 / 1500: loss 2.089661\n",
      "iteration 600 / 1500: loss 2.090127\n",
      "iteration 700 / 1500: loss 2.095867\n",
      "iteration 800 / 1500: loss 2.129903\n",
      "iteration 900 / 1500: loss 2.120056\n",
      "iteration 1000 / 1500: loss 2.130335\n",
      "iteration 1100 / 1500: loss 2.118408\n",
      "iteration 1200 / 1500: loss 2.176659\n",
      "iteration 1300 / 1500: loss 2.153686\n",
      "iteration 1400 / 1500: loss 2.128858\n",
      "\n",
      "Evaluating combo:  (1e-06, 35000.0)\n",
      "iteration 0 / 1500: loss 1089.538339\n",
      "iteration 100 / 1500: loss 2.114861\n",
      "iteration 200 / 1500: loss 2.140628\n",
      "iteration 300 / 1500: loss 2.102087\n",
      "iteration 400 / 1500: loss 2.128577\n",
      "iteration 500 / 1500: loss 2.105186\n",
      "iteration 600 / 1500: loss 2.146726\n",
      "iteration 700 / 1500: loss 2.189467\n",
      "iteration 800 / 1500: loss 2.180753\n",
      "iteration 900 / 1500: loss 2.106681\n",
      "iteration 1000 / 1500: loss 2.100397\n",
      "iteration 1100 / 1500: loss 2.103726\n",
      "iteration 1200 / 1500: loss 2.101894\n",
      "iteration 1300 / 1500: loss 2.127137\n",
      "iteration 1400 / 1500: loss 2.122635\n",
      "\n",
      "Evaluating combo:  (1e-06, 40000.0)\n",
      "iteration 0 / 1500: loss 1237.090594\n",
      "iteration 100 / 1500: loss 2.150199\n",
      "iteration 200 / 1500: loss 2.159736\n",
      "iteration 300 / 1500: loss 2.096556\n",
      "iteration 400 / 1500: loss 2.175222\n",
      "iteration 500 / 1500: loss 2.154733\n",
      "iteration 600 / 1500: loss 2.166594\n",
      "iteration 700 / 1500: loss 2.112978\n",
      "iteration 800 / 1500: loss 2.164990\n",
      "iteration 900 / 1500: loss 2.123009\n",
      "iteration 1000 / 1500: loss 2.160600\n",
      "iteration 1100 / 1500: loss 2.117364\n",
      "iteration 1200 / 1500: loss 2.088334\n",
      "iteration 1300 / 1500: loss 2.164018\n",
      "iteration 1400 / 1500: loss 2.133244\n",
      "\n",
      "Evaluating combo:  (1e-06, 45000.0)\n",
      "iteration 0 / 1500: loss 1368.546821\n",
      "iteration 100 / 1500: loss 2.084092\n",
      "iteration 200 / 1500: loss 2.170964\n",
      "iteration 300 / 1500: loss 2.120428\n",
      "iteration 400 / 1500: loss 2.144043\n",
      "iteration 500 / 1500: loss 2.139796\n",
      "iteration 600 / 1500: loss 2.167311\n",
      "iteration 700 / 1500: loss 2.141296\n",
      "iteration 800 / 1500: loss 2.183771\n",
      "iteration 900 / 1500: loss 2.137413\n",
      "iteration 1000 / 1500: loss 2.147687\n",
      "iteration 1100 / 1500: loss 2.115209\n",
      "iteration 1200 / 1500: loss 2.162837\n",
      "iteration 1300 / 1500: loss 2.115270\n",
      "iteration 1400 / 1500: loss 2.137302\n",
      "\n",
      "Evaluating combo:  (1e-06, 50000.0)\n",
      "iteration 0 / 1500: loss 1533.258403\n",
      "iteration 100 / 1500: loss 2.156756\n",
      "iteration 200 / 1500: loss 2.160951\n",
      "iteration 300 / 1500: loss 2.177421\n",
      "iteration 400 / 1500: loss 2.165885\n",
      "iteration 500 / 1500: loss 2.210195\n",
      "iteration 600 / 1500: loss 2.134550\n",
      "iteration 700 / 1500: loss 2.158029\n",
      "iteration 800 / 1500: loss 2.160143\n",
      "iteration 900 / 1500: loss 2.140147\n",
      "iteration 1000 / 1500: loss 2.156776\n",
      "iteration 1100 / 1500: loss 2.125431\n",
      "iteration 1200 / 1500: loss 2.117512\n",
      "iteration 1300 / 1500: loss 2.150736\n",
      "iteration 1400 / 1500: loss 2.155283\n",
      "\n",
      "Evaluating combo:  (2e-06, 1000.0)\n",
      "iteration 0 / 1500: loss 35.699174\n",
      "iteration 100 / 1500: loss 15.560205\n",
      "iteration 200 / 1500: loss 8.011288\n",
      "iteration 300 / 1500: loss 4.547364\n",
      "iteration 400 / 1500: loss 2.996849\n",
      "iteration 500 / 1500: loss 2.358333\n",
      "iteration 600 / 1500: loss 2.156025\n",
      "iteration 700 / 1500: loss 1.966911\n",
      "iteration 800 / 1500: loss 1.932098\n",
      "iteration 900 / 1500: loss 1.882405\n",
      "iteration 1000 / 1500: loss 1.918073\n",
      "iteration 1100 / 1500: loss 1.930376\n",
      "iteration 1200 / 1500: loss 1.742808\n",
      "iteration 1300 / 1500: loss 1.817165\n",
      "iteration 1400 / 1500: loss 1.752822\n",
      "\n",
      "Evaluating combo:  (2e-06, 2000.0)\n",
      "iteration 0 / 1500: loss 67.828333\n",
      "iteration 100 / 1500: loss 14.108457\n",
      "iteration 200 / 1500: loss 4.356786\n",
      "iteration 300 / 1500: loss 2.313627\n",
      "iteration 400 / 1500: loss 2.000238\n",
      "iteration 500 / 1500: loss 1.865950\n",
      "iteration 600 / 1500: loss 1.792490\n",
      "iteration 700 / 1500: loss 1.975354\n",
      "iteration 800 / 1500: loss 1.866409\n",
      "iteration 900 / 1500: loss 1.946105\n",
      "iteration 1000 / 1500: loss 1.902939\n",
      "iteration 1100 / 1500: loss 1.776885\n",
      "iteration 1200 / 1500: loss 1.874309\n",
      "iteration 1300 / 1500: loss 1.903345\n",
      "iteration 1400 / 1500: loss 1.854623\n",
      "\n",
      "Evaluating combo:  (2e-06, 3000.0)\n",
      "iteration 0 / 1500: loss 98.240241\n",
      "iteration 100 / 1500: loss 10.007173\n",
      "iteration 200 / 1500: loss 2.608460\n",
      "iteration 300 / 1500: loss 1.976859\n",
      "iteration 400 / 1500: loss 1.949627\n",
      "iteration 500 / 1500: loss 1.929847\n",
      "iteration 600 / 1500: loss 1.941589\n",
      "iteration 700 / 1500: loss 1.949362\n",
      "iteration 800 / 1500: loss 1.951563\n",
      "iteration 900 / 1500: loss 1.904932\n",
      "iteration 1000 / 1500: loss 2.002816\n",
      "iteration 1100 / 1500: loss 1.923158\n",
      "iteration 1200 / 1500: loss 1.789257\n",
      "iteration 1300 / 1500: loss 1.890028\n",
      "iteration 1400 / 1500: loss 1.938059\n",
      "\n",
      "Evaluating combo:  (2e-06, 4000.0)\n",
      "iteration 0 / 1500: loss 129.797693\n",
      "iteration 100 / 1500: loss 6.842204\n",
      "iteration 200 / 1500: loss 2.159358\n",
      "iteration 300 / 1500: loss 1.960052\n",
      "iteration 400 / 1500: loss 2.000841\n",
      "iteration 500 / 1500: loss 2.017200\n",
      "iteration 600 / 1500: loss 1.994652\n",
      "iteration 700 / 1500: loss 1.938884\n",
      "iteration 800 / 1500: loss 1.892030\n",
      "iteration 900 / 1500: loss 1.947424\n",
      "iteration 1000 / 1500: loss 1.949858\n",
      "iteration 1100 / 1500: loss 1.988442\n",
      "iteration 1200 / 1500: loss 2.057662\n",
      "iteration 1300 / 1500: loss 1.882998\n",
      "iteration 1400 / 1500: loss 1.967709\n",
      "\n",
      "Evaluating combo:  (2e-06, 5000.0)\n",
      "iteration 0 / 1500: loss 159.090499\n",
      "iteration 100 / 1500: loss 4.561459\n",
      "iteration 200 / 1500: loss 2.112000\n",
      "iteration 300 / 1500: loss 1.933258\n",
      "iteration 400 / 1500: loss 2.040483\n",
      "iteration 500 / 1500: loss 2.079200\n",
      "iteration 600 / 1500: loss 2.097534\n",
      "iteration 700 / 1500: loss 1.997062\n",
      "iteration 800 / 1500: loss 2.059593\n",
      "iteration 900 / 1500: loss 1.953888\n",
      "iteration 1000 / 1500: loss 1.920746\n",
      "iteration 1100 / 1500: loss 1.939925\n",
      "iteration 1200 / 1500: loss 1.959147\n",
      "iteration 1300 / 1500: loss 2.029801\n",
      "iteration 1400 / 1500: loss 1.978402\n",
      "\n",
      "Evaluating combo:  (2e-06, 10000.0)\n",
      "iteration 0 / 1500: loss 311.592388\n",
      "iteration 100 / 1500: loss 2.069234\n",
      "iteration 200 / 1500: loss 2.064779\n",
      "iteration 300 / 1500: loss 1.963680\n",
      "iteration 400 / 1500: loss 2.029157\n",
      "iteration 500 / 1500: loss 2.125369\n",
      "iteration 600 / 1500: loss 2.005917\n",
      "iteration 700 / 1500: loss 1.993143\n",
      "iteration 800 / 1500: loss 2.047497\n",
      "iteration 900 / 1500: loss 2.066353\n",
      "iteration 1000 / 1500: loss 2.059447\n",
      "iteration 1100 / 1500: loss 2.001739\n",
      "iteration 1200 / 1500: loss 1.941586\n",
      "iteration 1300 / 1500: loss 2.053912\n",
      "iteration 1400 / 1500: loss 2.054107\n",
      "\n",
      "Evaluating combo:  (2e-06, 20000.0)\n",
      "iteration 0 / 1500: loss 626.475289\n",
      "iteration 100 / 1500: loss 2.057780\n",
      "iteration 200 / 1500: loss 2.137943\n",
      "iteration 300 / 1500: loss 2.133248\n",
      "iteration 400 / 1500: loss 2.069013\n",
      "iteration 500 / 1500: loss 2.111372\n",
      "iteration 600 / 1500: loss 2.060313\n",
      "iteration 700 / 1500: loss 2.166587\n",
      "iteration 800 / 1500: loss 2.088709\n",
      "iteration 900 / 1500: loss 2.032437\n",
      "iteration 1000 / 1500: loss 2.105049\n",
      "iteration 1100 / 1500: loss 2.121168\n",
      "iteration 1200 / 1500: loss 2.171552\n",
      "iteration 1300 / 1500: loss 2.110641\n",
      "iteration 1400 / 1500: loss 2.120858\n",
      "\n",
      "Evaluating combo:  (2e-06, 25000.0)\n",
      "iteration 0 / 1500: loss 772.326591\n",
      "iteration 100 / 1500: loss 2.138391\n",
      "iteration 200 / 1500: loss 2.031796\n",
      "iteration 300 / 1500: loss 2.082705\n",
      "iteration 400 / 1500: loss 2.060569\n",
      "iteration 500 / 1500: loss 2.123248\n",
      "iteration 600 / 1500: loss 2.089547\n",
      "iteration 700 / 1500: loss 2.105190\n",
      "iteration 800 / 1500: loss 2.140983\n",
      "iteration 900 / 1500: loss 2.097710\n",
      "iteration 1000 / 1500: loss 2.169610\n",
      "iteration 1100 / 1500: loss 2.109562\n",
      "iteration 1200 / 1500: loss 2.097381\n",
      "iteration 1300 / 1500: loss 2.127732\n",
      "iteration 1400 / 1500: loss 2.113613\n",
      "\n",
      "Evaluating combo:  (2e-06, 30000.0)\n",
      "iteration 0 / 1500: loss 921.414681\n",
      "iteration 100 / 1500: loss 2.106442\n",
      "iteration 200 / 1500: loss 2.148054\n",
      "iteration 300 / 1500: loss 2.144860\n",
      "iteration 400 / 1500: loss 2.153475\n",
      "iteration 500 / 1500: loss 2.090545\n",
      "iteration 600 / 1500: loss 2.148694\n",
      "iteration 700 / 1500: loss 2.147696\n",
      "iteration 800 / 1500: loss 2.108553\n",
      "iteration 900 / 1500: loss 2.091442\n",
      "iteration 1000 / 1500: loss 2.131071\n",
      "iteration 1100 / 1500: loss 2.198576\n",
      "iteration 1200 / 1500: loss 2.113928\n",
      "iteration 1300 / 1500: loss 2.124822\n",
      "iteration 1400 / 1500: loss 2.098614\n",
      "\n",
      "Evaluating combo:  (2e-06, 35000.0)\n",
      "iteration 0 / 1500: loss 1081.277634\n",
      "iteration 100 / 1500: loss 2.143822\n",
      "iteration 200 / 1500: loss 2.138586\n",
      "iteration 300 / 1500: loss 2.209807\n",
      "iteration 400 / 1500: loss 2.176383\n",
      "iteration 500 / 1500: loss 2.088048\n",
      "iteration 600 / 1500: loss 2.172766\n",
      "iteration 700 / 1500: loss 2.212601\n",
      "iteration 800 / 1500: loss 2.165560\n",
      "iteration 900 / 1500: loss 2.148837\n",
      "iteration 1000 / 1500: loss 2.156182\n",
      "iteration 1100 / 1500: loss 2.159943\n",
      "iteration 1200 / 1500: loss 2.072259\n",
      "iteration 1300 / 1500: loss 2.153712\n",
      "iteration 1400 / 1500: loss 2.174539\n",
      "\n",
      "Evaluating combo:  (2e-06, 40000.0)\n",
      "iteration 0 / 1500: loss 1238.615934\n",
      "iteration 100 / 1500: loss 2.231579\n",
      "iteration 200 / 1500: loss 2.236277\n",
      "iteration 300 / 1500: loss 2.214760\n",
      "iteration 400 / 1500: loss 2.194516\n",
      "iteration 500 / 1500: loss 2.186096\n",
      "iteration 600 / 1500: loss 2.187183\n",
      "iteration 700 / 1500: loss 2.190853\n",
      "iteration 800 / 1500: loss 2.123507\n",
      "iteration 900 / 1500: loss 2.177222\n",
      "iteration 1000 / 1500: loss 2.152446\n",
      "iteration 1100 / 1500: loss 2.199283\n",
      "iteration 1200 / 1500: loss 2.179762\n",
      "iteration 1300 / 1500: loss 2.146515\n",
      "iteration 1400 / 1500: loss 2.202784\n",
      "\n",
      "Evaluating combo:  (2e-06, 45000.0)\n",
      "iteration 0 / 1500: loss 1381.511931\n",
      "iteration 100 / 1500: loss 2.226280\n",
      "iteration 200 / 1500: loss 2.139247\n",
      "iteration 300 / 1500: loss 2.150837\n",
      "iteration 400 / 1500: loss 2.115835\n",
      "iteration 500 / 1500: loss 2.111248\n",
      "iteration 600 / 1500: loss 2.227976\n",
      "iteration 700 / 1500: loss 2.193813\n",
      "iteration 800 / 1500: loss 2.121487\n",
      "iteration 900 / 1500: loss 2.069098\n",
      "iteration 1000 / 1500: loss 2.111911\n",
      "iteration 1100 / 1500: loss 2.112898\n",
      "iteration 1200 / 1500: loss 2.205019\n",
      "iteration 1300 / 1500: loss 2.155346\n",
      "iteration 1400 / 1500: loss 2.150879\n",
      "\n",
      "Evaluating combo:  (2e-06, 50000.0)\n",
      "iteration 0 / 1500: loss 1559.287490\n",
      "iteration 100 / 1500: loss 2.161812\n",
      "iteration 200 / 1500: loss 2.206288\n",
      "iteration 300 / 1500: loss 2.138310\n",
      "iteration 400 / 1500: loss 2.197938\n",
      "iteration 500 / 1500: loss 2.179032\n",
      "iteration 600 / 1500: loss 2.174546\n",
      "iteration 700 / 1500: loss 2.157220\n",
      "iteration 800 / 1500: loss 2.193052\n",
      "iteration 900 / 1500: loss 2.165990\n",
      "iteration 1000 / 1500: loss 2.159318\n",
      "iteration 1100 / 1500: loss 2.201176\n",
      "iteration 1200 / 1500: loss 2.201220\n",
      "iteration 1300 / 1500: loss 2.196951\n",
      "iteration 1400 / 1500: loss 2.191650\n",
      "\n",
      "Evaluating combo:  (3e-06, 1000.0)\n",
      "iteration 0 / 1500: loss 36.203646\n",
      "iteration 100 / 1500: loss 10.993666\n",
      "iteration 200 / 1500: loss 4.464282\n",
      "iteration 300 / 1500: loss 2.604504\n",
      "iteration 400 / 1500: loss 1.999813\n",
      "iteration 500 / 1500: loss 1.938243\n",
      "iteration 600 / 1500: loss 1.830417\n",
      "iteration 700 / 1500: loss 1.735765\n",
      "iteration 800 / 1500: loss 2.008881\n",
      "iteration 900 / 1500: loss 1.872280\n",
      "iteration 1000 / 1500: loss 1.861677\n",
      "iteration 1100 / 1500: loss 1.929276\n",
      "iteration 1200 / 1500: loss 1.927633\n",
      "iteration 1300 / 1500: loss 1.732515\n",
      "iteration 1400 / 1500: loss 1.856347\n",
      "\n",
      "Evaluating combo:  (3e-06, 2000.0)\n",
      "iteration 0 / 1500: loss 68.136285\n",
      "iteration 100 / 1500: loss 7.385461\n",
      "iteration 200 / 1500: loss 2.366576\n",
      "iteration 300 / 1500: loss 1.967704\n",
      "iteration 400 / 1500: loss 1.972806\n",
      "iteration 500 / 1500: loss 1.918698\n",
      "iteration 600 / 1500: loss 1.949388\n",
      "iteration 700 / 1500: loss 1.953093\n",
      "iteration 800 / 1500: loss 1.952343\n",
      "iteration 900 / 1500: loss 1.815209\n",
      "iteration 1000 / 1500: loss 1.897510\n",
      "iteration 1100 / 1500: loss 1.881307\n",
      "iteration 1200 / 1500: loss 1.921762\n",
      "iteration 1300 / 1500: loss 1.912580\n",
      "iteration 1400 / 1500: loss 1.945315\n",
      "\n",
      "Evaluating combo:  (3e-06, 3000.0)\n",
      "iteration 0 / 1500: loss 97.076027\n",
      "iteration 100 / 1500: loss 4.271116\n",
      "iteration 200 / 1500: loss 2.087284\n",
      "iteration 300 / 1500: loss 1.922924\n",
      "iteration 400 / 1500: loss 1.886280\n",
      "iteration 500 / 1500: loss 2.012399\n",
      "iteration 600 / 1500: loss 1.897125\n",
      "iteration 700 / 1500: loss 1.949797\n",
      "iteration 800 / 1500: loss 1.908641\n",
      "iteration 900 / 1500: loss 1.865714\n",
      "iteration 1000 / 1500: loss 1.885690\n",
      "iteration 1100 / 1500: loss 1.992147\n",
      "iteration 1200 / 1500: loss 2.017382\n",
      "iteration 1300 / 1500: loss 1.903800\n",
      "iteration 1400 / 1500: loss 1.974389\n",
      "\n",
      "Evaluating combo:  (3e-06, 4000.0)\n",
      "iteration 0 / 1500: loss 130.015089\n",
      "iteration 100 / 1500: loss 3.000186\n",
      "iteration 200 / 1500: loss 1.933133\n",
      "iteration 300 / 1500: loss 2.013187\n",
      "iteration 400 / 1500: loss 1.920458\n",
      "iteration 500 / 1500: loss 1.972853\n",
      "iteration 600 / 1500: loss 1.883893\n",
      "iteration 700 / 1500: loss 2.065283\n",
      "iteration 800 / 1500: loss 1.987837\n",
      "iteration 900 / 1500: loss 1.856176\n",
      "iteration 1000 / 1500: loss 2.050813\n",
      "iteration 1100 / 1500: loss 2.042927\n",
      "iteration 1200 / 1500: loss 1.966460\n",
      "iteration 1300 / 1500: loss 1.989256\n",
      "iteration 1400 / 1500: loss 1.873319\n",
      "\n",
      "Evaluating combo:  (3e-06, 5000.0)\n",
      "iteration 0 / 1500: loss 162.486243\n",
      "iteration 100 / 1500: loss 2.381016\n",
      "iteration 200 / 1500: loss 2.038935\n",
      "iteration 300 / 1500: loss 2.005074\n",
      "iteration 400 / 1500: loss 1.920381\n",
      "iteration 500 / 1500: loss 1.989704\n",
      "iteration 600 / 1500: loss 1.957069\n",
      "iteration 700 / 1500: loss 2.025893\n",
      "iteration 800 / 1500: loss 2.003486\n",
      "iteration 900 / 1500: loss 2.032160\n",
      "iteration 1000 / 1500: loss 2.010471\n",
      "iteration 1100 / 1500: loss 1.992126\n",
      "iteration 1200 / 1500: loss 2.001713\n",
      "iteration 1300 / 1500: loss 1.981891\n",
      "iteration 1400 / 1500: loss 1.968619\n",
      "\n",
      "Evaluating combo:  (3e-06, 10000.0)\n",
      "iteration 0 / 1500: loss 311.581180\n",
      "iteration 100 / 1500: loss 2.035238\n",
      "iteration 200 / 1500: loss 2.026444\n",
      "iteration 300 / 1500: loss 2.089094\n",
      "iteration 400 / 1500: loss 2.096612\n",
      "iteration 500 / 1500: loss 2.049002\n",
      "iteration 600 / 1500: loss 2.112369\n",
      "iteration 700 / 1500: loss 1.984630\n",
      "iteration 800 / 1500: loss 2.137230\n",
      "iteration 900 / 1500: loss 2.032494\n",
      "iteration 1000 / 1500: loss 2.075386\n",
      "iteration 1100 / 1500: loss 1.961485\n",
      "iteration 1200 / 1500: loss 2.085912\n",
      "iteration 1300 / 1500: loss 2.091429\n",
      "iteration 1400 / 1500: loss 2.065437\n",
      "\n",
      "Evaluating combo:  (3e-06, 20000.0)\n",
      "iteration 0 / 1500: loss 624.264126\n",
      "iteration 100 / 1500: loss 2.077372\n",
      "iteration 200 / 1500: loss 2.126559\n",
      "iteration 300 / 1500: loss 2.057173\n",
      "iteration 400 / 1500: loss 2.062525\n",
      "iteration 500 / 1500: loss 2.090410\n",
      "iteration 600 / 1500: loss 2.087216\n",
      "iteration 700 / 1500: loss 2.095359\n",
      "iteration 800 / 1500: loss 2.102359\n",
      "iteration 900 / 1500: loss 2.142564\n",
      "iteration 1000 / 1500: loss 2.009047\n",
      "iteration 1100 / 1500: loss 2.123241\n",
      "iteration 1200 / 1500: loss 2.212291\n",
      "iteration 1300 / 1500: loss 2.153389\n",
      "iteration 1400 / 1500: loss 2.162063\n",
      "\n",
      "Evaluating combo:  (3e-06, 25000.0)\n",
      "iteration 0 / 1500: loss 777.828313\n",
      "iteration 100 / 1500: loss 2.130648\n",
      "iteration 200 / 1500: loss 2.162440\n",
      "iteration 300 / 1500: loss 2.110262\n",
      "iteration 400 / 1500: loss 2.084090\n",
      "iteration 500 / 1500: loss 2.065972\n",
      "iteration 600 / 1500: loss 2.161671\n",
      "iteration 700 / 1500: loss 2.090094\n",
      "iteration 800 / 1500: loss 2.155293\n",
      "iteration 900 / 1500: loss 2.167586\n",
      "iteration 1000 / 1500: loss 2.213321\n",
      "iteration 1100 / 1500: loss 2.112700\n",
      "iteration 1200 / 1500: loss 2.128968\n",
      "iteration 1300 / 1500: loss 2.071462\n",
      "iteration 1400 / 1500: loss 2.165642\n",
      "\n",
      "Evaluating combo:  (3e-06, 30000.0)\n",
      "iteration 0 / 1500: loss 928.528916\n",
      "iteration 100 / 1500: loss 2.163047\n",
      "iteration 200 / 1500: loss 2.164649\n",
      "iteration 300 / 1500: loss 2.198141\n",
      "iteration 400 / 1500: loss 2.169087\n",
      "iteration 500 / 1500: loss 2.200679\n",
      "iteration 600 / 1500: loss 2.145327\n",
      "iteration 700 / 1500: loss 2.112605\n",
      "iteration 800 / 1500: loss 2.128419\n",
      "iteration 900 / 1500: loss 2.211381\n",
      "iteration 1000 / 1500: loss 2.106198\n",
      "iteration 1100 / 1500: loss 2.152450\n",
      "iteration 1200 / 1500: loss 2.140754\n",
      "iteration 1300 / 1500: loss 2.262048\n",
      "iteration 1400 / 1500: loss 2.166450\n",
      "\n",
      "Evaluating combo:  (3e-06, 35000.0)\n",
      "iteration 0 / 1500: loss 1084.525388\n",
      "iteration 100 / 1500: loss 2.151507\n",
      "iteration 200 / 1500: loss 2.217757\n",
      "iteration 300 / 1500: loss 2.145513\n",
      "iteration 400 / 1500: loss 2.166986\n",
      "iteration 500 / 1500: loss 2.155752\n",
      "iteration 600 / 1500: loss 2.164659\n",
      "iteration 700 / 1500: loss 2.192957\n",
      "iteration 800 / 1500: loss 2.192655\n",
      "iteration 900 / 1500: loss 2.211809\n",
      "iteration 1000 / 1500: loss 2.225007\n",
      "iteration 1100 / 1500: loss 2.172697\n",
      "iteration 1200 / 1500: loss 2.191378\n",
      "iteration 1300 / 1500: loss 2.196412\n",
      "iteration 1400 / 1500: loss 2.132343\n",
      "\n",
      "Evaluating combo:  (3e-06, 40000.0)\n",
      "iteration 0 / 1500: loss 1244.868073\n",
      "iteration 100 / 1500: loss 2.227516\n",
      "iteration 200 / 1500: loss 2.174058\n",
      "iteration 300 / 1500: loss 2.189276\n",
      "iteration 400 / 1500: loss 2.204254\n",
      "iteration 500 / 1500: loss 2.175364\n",
      "iteration 600 / 1500: loss 2.212313\n",
      "iteration 700 / 1500: loss 2.113201\n",
      "iteration 800 / 1500: loss 2.217039\n",
      "iteration 900 / 1500: loss 2.187603\n",
      "iteration 1000 / 1500: loss 2.212046\n",
      "iteration 1100 / 1500: loss 2.140205\n",
      "iteration 1200 / 1500: loss 2.193882\n",
      "iteration 1300 / 1500: loss 2.162463\n",
      "iteration 1400 / 1500: loss 2.206396\n",
      "\n",
      "Evaluating combo:  (3e-06, 45000.0)\n",
      "iteration 0 / 1500: loss 1383.014922\n",
      "iteration 100 / 1500: loss 2.188634\n",
      "iteration 200 / 1500: loss 2.177513\n",
      "iteration 300 / 1500: loss 2.180648\n",
      "iteration 400 / 1500: loss 2.199124\n",
      "iteration 500 / 1500: loss 2.232153\n",
      "iteration 600 / 1500: loss 2.211093\n",
      "iteration 700 / 1500: loss 2.233530\n",
      "iteration 800 / 1500: loss 2.180172\n",
      "iteration 900 / 1500: loss 2.180666\n",
      "iteration 1000 / 1500: loss 2.196549\n",
      "iteration 1100 / 1500: loss 2.222263\n",
      "iteration 1200 / 1500: loss 2.212686\n",
      "iteration 1300 / 1500: loss 2.161659\n",
      "iteration 1400 / 1500: loss 2.235233\n",
      "\n",
      "Evaluating combo:  (3e-06, 50000.0)\n",
      "iteration 0 / 1500: loss 1539.881370\n",
      "iteration 100 / 1500: loss 2.243942\n",
      "iteration 200 / 1500: loss 2.152120\n",
      "iteration 300 / 1500: loss 2.194930\n",
      "iteration 400 / 1500: loss 2.262080\n",
      "iteration 500 / 1500: loss 2.163806\n",
      "iteration 600 / 1500: loss 2.253285\n",
      "iteration 700 / 1500: loss 2.338523\n",
      "iteration 800 / 1500: loss 2.251381\n",
      "iteration 900 / 1500: loss 2.265570\n",
      "iteration 1000 / 1500: loss 2.150323\n",
      "iteration 1100 / 1500: loss 2.185699\n",
      "iteration 1200 / 1500: loss 2.158793\n",
      "iteration 1300 / 1500: loss 2.240636\n",
      "iteration 1400 / 1500: loss 2.241396\n",
      "\n",
      "Evaluating combo:  (4e-06, 1000.0)\n",
      "iteration 0 / 1500: loss 36.580901\n",
      "iteration 100 / 1500: loss 7.914036\n",
      "iteration 200 / 1500: loss 3.006789\n",
      "iteration 300 / 1500: loss 2.039099\n",
      "iteration 400 / 1500: loss 2.010872\n",
      "iteration 500 / 1500: loss 1.935990\n",
      "iteration 600 / 1500: loss 1.815164\n",
      "iteration 700 / 1500: loss 1.933877\n",
      "iteration 800 / 1500: loss 1.940186\n",
      "iteration 900 / 1500: loss 1.861501\n",
      "iteration 1000 / 1500: loss 1.859456\n",
      "iteration 1100 / 1500: loss 1.943708\n",
      "iteration 1200 / 1500: loss 1.885081\n",
      "iteration 1300 / 1500: loss 1.914256\n",
      "iteration 1400 / 1500: loss 1.797521\n",
      "\n",
      "Evaluating combo:  (4e-06, 2000.0)\n",
      "iteration 0 / 1500: loss 67.187609\n",
      "iteration 100 / 1500: loss 4.313164\n",
      "iteration 200 / 1500: loss 2.018363\n",
      "iteration 300 / 1500: loss 1.840079\n",
      "iteration 400 / 1500: loss 1.865523\n",
      "iteration 500 / 1500: loss 1.951244\n",
      "iteration 600 / 1500: loss 1.997486\n",
      "iteration 700 / 1500: loss 1.952568\n",
      "iteration 800 / 1500: loss 2.043428\n",
      "iteration 900 / 1500: loss 1.911310\n",
      "iteration 1000 / 1500: loss 1.835424\n",
      "iteration 1100 / 1500: loss 1.847796\n",
      "iteration 1200 / 1500: loss 1.813810\n",
      "iteration 1300 / 1500: loss 1.933629\n",
      "iteration 1400 / 1500: loss 1.930413\n",
      "\n",
      "Evaluating combo:  (4e-06, 3000.0)\n",
      "iteration 0 / 1500: loss 99.236396\n",
      "iteration 100 / 1500: loss 2.640377\n",
      "iteration 200 / 1500: loss 1.889422\n",
      "iteration 300 / 1500: loss 1.950435\n",
      "iteration 400 / 1500: loss 2.035398\n",
      "iteration 500 / 1500: loss 2.015685\n",
      "iteration 600 / 1500: loss 1.975627\n",
      "iteration 700 / 1500: loss 1.976040\n",
      "iteration 800 / 1500: loss 2.028111\n",
      "iteration 900 / 1500: loss 1.958795\n",
      "iteration 1000 / 1500: loss 2.000437\n",
      "iteration 1100 / 1500: loss 2.000626\n",
      "iteration 1200 / 1500: loss 2.003134\n",
      "iteration 1300 / 1500: loss 1.960828\n",
      "iteration 1400 / 1500: loss 2.011883\n",
      "\n",
      "Evaluating combo:  (4e-06, 4000.0)\n",
      "iteration 0 / 1500: loss 130.509584\n",
      "iteration 100 / 1500: loss 2.216188\n",
      "iteration 200 / 1500: loss 2.016903\n",
      "iteration 300 / 1500: loss 1.952550\n",
      "iteration 400 / 1500: loss 1.882826\n",
      "iteration 500 / 1500: loss 2.026099\n",
      "iteration 600 / 1500: loss 1.999948\n",
      "iteration 700 / 1500: loss 2.035461\n",
      "iteration 800 / 1500: loss 1.923130\n",
      "iteration 900 / 1500: loss 1.930878\n",
      "iteration 1000 / 1500: loss 1.883064\n",
      "iteration 1100 / 1500: loss 1.946081\n",
      "iteration 1200 / 1500: loss 1.948681\n",
      "iteration 1300 / 1500: loss 1.930651\n",
      "iteration 1400 / 1500: loss 1.989974\n",
      "\n",
      "Evaluating combo:  (4e-06, 5000.0)\n",
      "iteration 0 / 1500: loss 158.934180\n",
      "iteration 100 / 1500: loss 2.023607\n",
      "iteration 200 / 1500: loss 2.038613\n",
      "iteration 300 / 1500: loss 2.032777\n",
      "iteration 400 / 1500: loss 1.971558\n",
      "iteration 500 / 1500: loss 2.010540\n",
      "iteration 600 / 1500: loss 1.995999\n",
      "iteration 700 / 1500: loss 2.058240\n",
      "iteration 800 / 1500: loss 2.013963\n",
      "iteration 900 / 1500: loss 1.940166\n",
      "iteration 1000 / 1500: loss 2.057723\n",
      "iteration 1100 / 1500: loss 1.981090\n",
      "iteration 1200 / 1500: loss 2.052755\n",
      "iteration 1300 / 1500: loss 2.104715\n",
      "iteration 1400 / 1500: loss 2.012339\n",
      "\n",
      "Evaluating combo:  (4e-06, 10000.0)\n",
      "iteration 0 / 1500: loss 310.901283\n",
      "iteration 100 / 1500: loss 2.171352\n",
      "iteration 200 / 1500: loss 2.157468\n",
      "iteration 300 / 1500: loss 2.042714\n",
      "iteration 400 / 1500: loss 2.064704\n",
      "iteration 500 / 1500: loss 2.056473\n",
      "iteration 600 / 1500: loss 2.130619\n",
      "iteration 700 / 1500: loss 2.124138\n",
      "iteration 800 / 1500: loss 2.079938\n",
      "iteration 900 / 1500: loss 2.007359\n",
      "iteration 1000 / 1500: loss 2.097158\n",
      "iteration 1100 / 1500: loss 2.147247\n",
      "iteration 1200 / 1500: loss 2.214899\n",
      "iteration 1300 / 1500: loss 2.109792\n",
      "iteration 1400 / 1500: loss 2.128038\n",
      "\n",
      "Evaluating combo:  (4e-06, 20000.0)\n",
      "iteration 0 / 1500: loss 625.111695\n",
      "iteration 100 / 1500: loss 2.102755\n",
      "iteration 200 / 1500: loss 2.183380\n",
      "iteration 300 / 1500: loss 2.150202\n",
      "iteration 400 / 1500: loss 2.128176\n",
      "iteration 500 / 1500: loss 2.158856\n",
      "iteration 600 / 1500: loss 2.202604\n",
      "iteration 700 / 1500: loss 2.192305\n",
      "iteration 800 / 1500: loss 2.093888\n",
      "iteration 900 / 1500: loss 2.168701\n",
      "iteration 1000 / 1500: loss 2.312076\n",
      "iteration 1100 / 1500: loss 2.205386\n",
      "iteration 1200 / 1500: loss 2.248361\n",
      "iteration 1300 / 1500: loss 2.104101\n",
      "iteration 1400 / 1500: loss 2.170913\n",
      "\n",
      "Evaluating combo:  (4e-06, 25000.0)\n",
      "iteration 0 / 1500: loss 763.657833\n",
      "iteration 100 / 1500: loss 2.176436\n",
      "iteration 200 / 1500: loss 2.189250\n",
      "iteration 300 / 1500: loss 2.155563\n",
      "iteration 400 / 1500: loss 2.218121\n",
      "iteration 500 / 1500: loss 2.132226\n",
      "iteration 600 / 1500: loss 2.184749\n",
      "iteration 700 / 1500: loss 2.244316\n",
      "iteration 800 / 1500: loss 2.200878\n",
      "iteration 900 / 1500: loss 2.267922\n",
      "iteration 1000 / 1500: loss 2.269301\n",
      "iteration 1100 / 1500: loss 2.250444\n",
      "iteration 1200 / 1500: loss 2.227197\n",
      "iteration 1300 / 1500: loss 2.119135\n",
      "iteration 1400 / 1500: loss 2.231895\n",
      "\n",
      "Evaluating combo:  (4e-06, 30000.0)\n",
      "iteration 0 / 1500: loss 922.689949\n",
      "iteration 100 / 1500: loss 2.323270\n",
      "iteration 200 / 1500: loss 2.330571\n",
      "iteration 300 / 1500: loss 2.201282\n",
      "iteration 400 / 1500: loss 2.127436\n",
      "iteration 500 / 1500: loss 2.285425\n",
      "iteration 600 / 1500: loss 2.238643\n",
      "iteration 700 / 1500: loss 2.146164\n",
      "iteration 800 / 1500: loss 2.189004\n",
      "iteration 900 / 1500: loss 2.292299\n",
      "iteration 1000 / 1500: loss 2.132643\n",
      "iteration 1100 / 1500: loss 2.271131\n",
      "iteration 1200 / 1500: loss 2.243611\n",
      "iteration 1300 / 1500: loss 2.124270\n",
      "iteration 1400 / 1500: loss 2.211013\n",
      "\n",
      "Evaluating combo:  (4e-06, 35000.0)\n",
      "iteration 0 / 1500: loss 1062.920416\n",
      "iteration 100 / 1500: loss 2.382884\n",
      "iteration 200 / 1500: loss 2.203006\n",
      "iteration 300 / 1500: loss 2.294290\n",
      "iteration 400 / 1500: loss 2.216601\n",
      "iteration 500 / 1500: loss 2.294986\n",
      "iteration 600 / 1500: loss 2.244530\n",
      "iteration 700 / 1500: loss 2.244540\n",
      "iteration 800 / 1500: loss 2.234317\n",
      "iteration 900 / 1500: loss 2.301373\n",
      "iteration 1000 / 1500: loss 2.432421\n",
      "iteration 1100 / 1500: loss 2.257461\n",
      "iteration 1200 / 1500: loss 2.260838\n",
      "iteration 1300 / 1500: loss 2.259614\n",
      "iteration 1400 / 1500: loss 2.188461\n",
      "\n",
      "Evaluating combo:  (4e-06, 40000.0)\n",
      "iteration 0 / 1500: loss 1224.190403\n",
      "iteration 100 / 1500: loss 2.299064\n",
      "iteration 200 / 1500: loss 2.165870\n",
      "iteration 300 / 1500: loss 2.281768\n",
      "iteration 400 / 1500: loss 2.314012\n",
      "iteration 500 / 1500: loss 2.158372\n",
      "iteration 600 / 1500: loss 2.226999\n",
      "iteration 700 / 1500: loss 2.230217\n",
      "iteration 800 / 1500: loss 2.356055\n",
      "iteration 900 / 1500: loss 2.225401\n",
      "iteration 1000 / 1500: loss 2.190874\n",
      "iteration 1100 / 1500: loss 2.266416\n",
      "iteration 1200 / 1500: loss 2.220912\n",
      "iteration 1300 / 1500: loss 2.374408\n",
      "iteration 1400 / 1500: loss 2.250121\n",
      "\n",
      "Evaluating combo:  (4e-06, 45000.0)\n",
      "iteration 0 / 1500: loss 1386.085222\n",
      "iteration 100 / 1500: loss 2.743465\n",
      "iteration 200 / 1500: loss 2.334849\n",
      "iteration 300 / 1500: loss 2.403489\n",
      "iteration 400 / 1500: loss 2.431074\n",
      "iteration 500 / 1500: loss 2.624202\n",
      "iteration 600 / 1500: loss 2.310302\n",
      "iteration 700 / 1500: loss 2.238338\n",
      "iteration 800 / 1500: loss 2.405032\n",
      "iteration 900 / 1500: loss 2.258264\n",
      "iteration 1000 / 1500: loss 2.272168\n",
      "iteration 1100 / 1500: loss 2.235146\n",
      "iteration 1200 / 1500: loss 2.303813\n",
      "iteration 1300 / 1500: loss 2.323529\n",
      "iteration 1400 / 1500: loss 2.595185\n",
      "\n",
      "Evaluating combo:  (4e-06, 50000.0)\n",
      "iteration 0 / 1500: loss 1525.390246\n",
      "iteration 100 / 1500: loss 2.324888\n",
      "iteration 200 / 1500: loss 2.278452\n",
      "iteration 300 / 1500: loss 2.386858\n",
      "iteration 400 / 1500: loss 2.716397\n",
      "iteration 500 / 1500: loss 2.337602\n",
      "iteration 600 / 1500: loss 2.574403\n",
      "iteration 700 / 1500: loss 2.348403\n",
      "iteration 800 / 1500: loss 2.291420\n",
      "iteration 900 / 1500: loss 2.264261\n",
      "iteration 1000 / 1500: loss 2.353589\n",
      "iteration 1100 / 1500: loss 2.305564\n",
      "iteration 1200 / 1500: loss 2.309369\n",
      "iteration 1300 / 1500: loss 2.368343\n",
      "iteration 1400 / 1500: loss 2.282726\n",
      "\n",
      "Evaluating combo:  (5e-06, 1000.0)\n",
      "iteration 0 / 1500: loss 36.175603\n",
      "iteration 100 / 1500: loss 5.887063\n",
      "iteration 200 / 1500: loss 2.601219\n",
      "iteration 300 / 1500: loss 2.134081\n",
      "iteration 400 / 1500: loss 1.869472\n",
      "iteration 500 / 1500: loss 2.187445\n",
      "iteration 600 / 1500: loss 1.896520\n",
      "iteration 700 / 1500: loss 1.896929\n",
      "iteration 800 / 1500: loss 2.145968\n",
      "iteration 900 / 1500: loss 2.198862\n",
      "iteration 1000 / 1500: loss 1.945371\n",
      "iteration 1100 / 1500: loss 1.968881\n",
      "iteration 1200 / 1500: loss 1.865549\n",
      "iteration 1300 / 1500: loss 1.863605\n",
      "iteration 1400 / 1500: loss 1.824220\n",
      "\n",
      "Evaluating combo:  (5e-06, 2000.0)\n",
      "iteration 0 / 1500: loss 68.365622\n",
      "iteration 100 / 1500: loss 3.026953\n",
      "iteration 200 / 1500: loss 1.979243\n",
      "iteration 300 / 1500: loss 1.970277\n",
      "iteration 400 / 1500: loss 1.992489\n",
      "iteration 500 / 1500: loss 2.106496\n",
      "iteration 600 / 1500: loss 1.966854\n",
      "iteration 700 / 1500: loss 2.019763\n",
      "iteration 800 / 1500: loss 2.058433\n",
      "iteration 900 / 1500: loss 1.896081\n",
      "iteration 1000 / 1500: loss 1.937356\n",
      "iteration 1100 / 1500: loss 2.059549\n",
      "iteration 1200 / 1500: loss 2.003927\n",
      "iteration 1300 / 1500: loss 2.186002\n",
      "iteration 1400 / 1500: loss 2.205790\n",
      "\n",
      "Evaluating combo:  (5e-06, 3000.0)\n",
      "iteration 0 / 1500: loss 98.272575\n",
      "iteration 100 / 1500: loss 2.303601\n",
      "iteration 200 / 1500: loss 2.014891\n",
      "iteration 300 / 1500: loss 1.925877\n",
      "iteration 400 / 1500: loss 1.979548\n",
      "iteration 500 / 1500: loss 2.129622\n",
      "iteration 600 / 1500: loss 2.192678\n",
      "iteration 700 / 1500: loss 2.214296\n",
      "iteration 800 / 1500: loss 2.071239\n",
      "iteration 900 / 1500: loss 1.956156\n",
      "iteration 1000 / 1500: loss 2.128348\n",
      "iteration 1100 / 1500: loss 2.073204\n",
      "iteration 1200 / 1500: loss 2.123703\n",
      "iteration 1300 / 1500: loss 2.021850\n",
      "iteration 1400 / 1500: loss 1.998535\n",
      "\n",
      "Evaluating combo:  (5e-06, 4000.0)\n",
      "iteration 0 / 1500: loss 130.050661\n",
      "iteration 100 / 1500: loss 2.188397\n",
      "iteration 200 / 1500: loss 2.263265\n",
      "iteration 300 / 1500: loss 2.330437\n",
      "iteration 400 / 1500: loss 2.160163\n",
      "iteration 500 / 1500: loss 2.054720\n",
      "iteration 600 / 1500: loss 2.042873\n",
      "iteration 700 / 1500: loss 2.150656\n",
      "iteration 800 / 1500: loss 2.079183\n",
      "iteration 900 / 1500: loss 2.046829\n",
      "iteration 1000 / 1500: loss 2.432312\n",
      "iteration 1100 / 1500: loss 2.044887\n",
      "iteration 1200 / 1500: loss 2.010619\n",
      "iteration 1300 / 1500: loss 2.101857\n",
      "iteration 1400 / 1500: loss 2.631883\n",
      "\n",
      "Evaluating combo:  (5e-06, 5000.0)\n",
      "iteration 0 / 1500: loss 159.332040\n",
      "iteration 100 / 1500: loss 2.006745\n",
      "iteration 200 / 1500: loss 2.070415\n",
      "iteration 300 / 1500: loss 2.533864\n",
      "iteration 400 / 1500: loss 2.086546\n",
      "iteration 500 / 1500: loss 2.045383\n",
      "iteration 600 / 1500: loss 2.351692\n",
      "iteration 700 / 1500: loss 2.172078\n",
      "iteration 800 / 1500: loss 2.378973\n",
      "iteration 900 / 1500: loss 2.224132\n",
      "iteration 1000 / 1500: loss 2.081955\n",
      "iteration 1100 / 1500: loss 2.080321\n",
      "iteration 1200 / 1500: loss 2.054990\n",
      "iteration 1300 / 1500: loss 2.035769\n",
      "iteration 1400 / 1500: loss 2.342330\n",
      "\n",
      "Evaluating combo:  (5e-06, 10000.0)\n",
      "iteration 0 / 1500: loss 307.641318\n",
      "iteration 100 / 1500: loss 2.188036\n",
      "iteration 200 / 1500: loss 2.068770\n",
      "iteration 300 / 1500: loss 2.018463\n",
      "iteration 400 / 1500: loss 2.232305\n",
      "iteration 500 / 1500: loss 2.186440\n",
      "iteration 600 / 1500: loss 2.140472\n",
      "iteration 700 / 1500: loss 2.277124\n",
      "iteration 800 / 1500: loss 2.566146\n",
      "iteration 900 / 1500: loss 2.181435\n",
      "iteration 1000 / 1500: loss 2.427001\n",
      "iteration 1100 / 1500: loss 2.137208\n",
      "iteration 1200 / 1500: loss 2.101407\n",
      "iteration 1300 / 1500: loss 2.335658\n",
      "iteration 1400 / 1500: loss 2.397927\n",
      "\n",
      "Evaluating combo:  (5e-06, 20000.0)\n",
      "iteration 0 / 1500: loss 618.276571\n",
      "iteration 100 / 1500: loss 2.533193\n",
      "iteration 200 / 1500: loss 2.224818\n",
      "iteration 300 / 1500: loss 2.384389\n",
      "iteration 400 / 1500: loss 2.317506\n",
      "iteration 500 / 1500: loss 2.693247\n",
      "iteration 600 / 1500: loss 2.239164\n",
      "iteration 700 / 1500: loss 2.344060\n",
      "iteration 800 / 1500: loss 2.815749\n",
      "iteration 900 / 1500: loss 2.955028\n",
      "iteration 1000 / 1500: loss 2.617545\n",
      "iteration 1100 / 1500: loss 2.210801\n",
      "iteration 1200 / 1500: loss 2.204642\n",
      "iteration 1300 / 1500: loss 2.557008\n",
      "iteration 1400 / 1500: loss 2.323084\n",
      "\n",
      "Evaluating combo:  (5e-06, 25000.0)\n",
      "iteration 0 / 1500: loss 778.251047\n",
      "iteration 100 / 1500: loss 2.601816\n",
      "iteration 200 / 1500: loss 2.444581\n",
      "iteration 300 / 1500: loss 3.248160\n",
      "iteration 400 / 1500: loss 3.411571\n",
      "iteration 500 / 1500: loss 2.301712\n",
      "iteration 600 / 1500: loss 2.561898\n",
      "iteration 700 / 1500: loss 2.740422\n",
      "iteration 800 / 1500: loss 2.794312\n",
      "iteration 900 / 1500: loss 2.287163\n",
      "iteration 1000 / 1500: loss 2.394384\n",
      "iteration 1100 / 1500: loss 2.819287\n",
      "iteration 1200 / 1500: loss 2.326605\n",
      "iteration 1300 / 1500: loss 2.549835\n",
      "iteration 1400 / 1500: loss 2.296283\n",
      "\n",
      "Evaluating combo:  (5e-06, 30000.0)\n",
      "iteration 0 / 1500: loss 915.444991\n",
      "iteration 100 / 1500: loss 2.896330\n",
      "iteration 200 / 1500: loss 2.699101\n",
      "iteration 300 / 1500: loss 2.287142\n",
      "iteration 400 / 1500: loss 2.389800\n",
      "iteration 500 / 1500: loss 2.837398\n",
      "iteration 600 / 1500: loss 2.999107\n",
      "iteration 700 / 1500: loss 2.666358\n",
      "iteration 800 / 1500: loss 2.360321\n",
      "iteration 900 / 1500: loss 2.398260\n",
      "iteration 1000 / 1500: loss 2.914393\n",
      "iteration 1100 / 1500: loss 2.691734\n",
      "iteration 1200 / 1500: loss 2.511300\n",
      "iteration 1300 / 1500: loss 2.223006\n",
      "iteration 1400 / 1500: loss 2.561049\n",
      "\n",
      "Evaluating combo:  (5e-06, 35000.0)\n",
      "iteration 0 / 1500: loss 1084.107858\n",
      "iteration 100 / 1500: loss 2.543881\n",
      "iteration 200 / 1500: loss 2.791611\n",
      "iteration 300 / 1500: loss 3.457183\n",
      "iteration 400 / 1500: loss 2.651006\n",
      "iteration 500 / 1500: loss 2.824653\n",
      "iteration 600 / 1500: loss 2.819495\n",
      "iteration 700 / 1500: loss 2.825834\n",
      "iteration 800 / 1500: loss 3.049942\n",
      "iteration 900 / 1500: loss 2.926527\n",
      "iteration 1000 / 1500: loss 2.808430\n",
      "iteration 1100 / 1500: loss 2.342306\n",
      "iteration 1200 / 1500: loss 2.706828\n",
      "iteration 1300 / 1500: loss 2.590219\n",
      "iteration 1400 / 1500: loss 2.967678\n",
      "\n",
      "Evaluating combo:  (5e-06, 40000.0)\n",
      "iteration 0 / 1500: loss 1240.270805\n",
      "iteration 100 / 1500: loss 3.547600\n",
      "iteration 200 / 1500: loss 3.323109\n",
      "iteration 300 / 1500: loss 2.853718\n",
      "iteration 400 / 1500: loss 2.788523\n",
      "iteration 500 / 1500: loss 3.389666\n",
      "iteration 600 / 1500: loss 2.557018\n",
      "iteration 700 / 1500: loss 2.645858\n",
      "iteration 800 / 1500: loss 2.660392\n",
      "iteration 900 / 1500: loss 3.856862\n",
      "iteration 1000 / 1500: loss 2.926606\n",
      "iteration 1100 / 1500: loss 2.736997\n",
      "iteration 1200 / 1500: loss 3.525191\n",
      "iteration 1300 / 1500: loss 2.932135\n",
      "iteration 1400 / 1500: loss 2.426841\n",
      "\n",
      "Evaluating combo:  (5e-06, 45000.0)\n",
      "iteration 0 / 1500: loss 1393.046435\n",
      "iteration 100 / 1500: loss 2.598175\n",
      "iteration 200 / 1500: loss 2.691458\n",
      "iteration 300 / 1500: loss 3.063971\n",
      "iteration 400 / 1500: loss 3.818770\n",
      "iteration 500 / 1500: loss 3.304422\n",
      "iteration 600 / 1500: loss 4.444168\n",
      "iteration 700 / 1500: loss 3.140261\n",
      "iteration 800 / 1500: loss 3.466697\n",
      "iteration 900 / 1500: loss 3.240969\n",
      "iteration 1000 / 1500: loss 2.759756\n",
      "iteration 1100 / 1500: loss 3.379572\n",
      "iteration 1200 / 1500: loss 3.330745\n",
      "iteration 1300 / 1500: loss 2.764416\n",
      "iteration 1400 / 1500: loss 3.372765\n",
      "\n",
      "Evaluating combo:  (5e-06, 50000.0)\n",
      "iteration 0 / 1500: loss 1535.405496\n",
      "iteration 100 / 1500: loss 3.447320\n",
      "iteration 200 / 1500: loss 3.469034\n",
      "iteration 300 / 1500: loss 3.343884\n",
      "iteration 400 / 1500: loss 3.184490\n",
      "iteration 500 / 1500: loss 2.854109\n",
      "iteration 600 / 1500: loss 3.767876\n",
      "iteration 700 / 1500: loss 3.643171\n",
      "iteration 800 / 1500: loss 4.657525\n",
      "iteration 900 / 1500: loss 3.706091\n",
      "iteration 1000 / 1500: loss 3.420590\n",
      "iteration 1100 / 1500: loss 3.005570\n",
      "iteration 1200 / 1500: loss 3.633658\n",
      "iteration 1300 / 1500: loss 3.338262\n",
      "iteration 1400 / 1500: loss 3.708906\n",
      "\n",
      "Evaluating combo:  (1e-05, 1000.0)\n",
      "iteration 0 / 1500: loss 35.525648\n",
      "iteration 100 / 1500: loss 2.861814\n",
      "iteration 200 / 1500: loss 3.219077\n",
      "iteration 300 / 1500: loss 3.566903\n",
      "iteration 400 / 1500: loss 3.813141\n",
      "iteration 500 / 1500: loss 3.258930\n",
      "iteration 600 / 1500: loss 2.525123\n",
      "iteration 700 / 1500: loss 4.261696\n",
      "iteration 800 / 1500: loss 2.775526\n",
      "iteration 900 / 1500: loss 4.285504\n",
      "iteration 1000 / 1500: loss 3.098710\n",
      "iteration 1100 / 1500: loss 2.666230\n",
      "iteration 1200 / 1500: loss 2.855873\n",
      "iteration 1300 / 1500: loss 2.574571\n",
      "iteration 1400 / 1500: loss 2.894956\n",
      "\n",
      "Evaluating combo:  (1e-05, 2000.0)\n",
      "iteration 0 / 1500: loss 65.771267\n",
      "iteration 100 / 1500: loss 3.204999\n",
      "iteration 200 / 1500: loss 3.884278\n",
      "iteration 300 / 1500: loss 4.478627\n",
      "iteration 400 / 1500: loss 3.515583\n",
      "iteration 500 / 1500: loss 3.850004\n",
      "iteration 600 / 1500: loss 5.250480\n",
      "iteration 700 / 1500: loss 4.109505\n",
      "iteration 800 / 1500: loss 3.129759\n",
      "iteration 900 / 1500: loss 2.963136\n",
      "iteration 1000 / 1500: loss 3.525164\n",
      "iteration 1100 / 1500: loss 2.995400\n",
      "iteration 1200 / 1500: loss 3.513749\n",
      "iteration 1300 / 1500: loss 4.532068\n",
      "iteration 1400 / 1500: loss 3.179622\n",
      "\n",
      "Evaluating combo:  (1e-05, 3000.0)\n",
      "iteration 0 / 1500: loss 96.609196\n",
      "iteration 100 / 1500: loss 3.865809\n",
      "iteration 200 / 1500: loss 2.623853\n",
      "iteration 300 / 1500: loss 4.226831\n",
      "iteration 400 / 1500: loss 3.773887\n",
      "iteration 500 / 1500: loss 3.218721\n",
      "iteration 600 / 1500: loss 3.663693\n",
      "iteration 700 / 1500: loss 4.309607\n",
      "iteration 800 / 1500: loss 3.932723\n",
      "iteration 900 / 1500: loss 4.617227\n",
      "iteration 1000 / 1500: loss 3.401960\n",
      "iteration 1100 / 1500: loss 3.449498\n",
      "iteration 1200 / 1500: loss 4.005541\n",
      "iteration 1300 / 1500: loss 3.035411\n",
      "iteration 1400 / 1500: loss 4.098903\n",
      "\n",
      "Evaluating combo:  (1e-05, 4000.0)\n",
      "iteration 0 / 1500: loss 129.821634\n",
      "iteration 100 / 1500: loss 3.485769\n",
      "iteration 200 / 1500: loss 2.764374\n",
      "iteration 300 / 1500: loss 4.866107\n",
      "iteration 400 / 1500: loss 3.309703\n",
      "iteration 500 / 1500: loss 3.522688\n",
      "iteration 600 / 1500: loss 2.860651\n",
      "iteration 700 / 1500: loss 3.092115\n",
      "iteration 800 / 1500: loss 4.242212\n",
      "iteration 900 / 1500: loss 3.602375\n",
      "iteration 1000 / 1500: loss 3.638731\n",
      "iteration 1100 / 1500: loss 4.384200\n",
      "iteration 1200 / 1500: loss 2.932871\n",
      "iteration 1300 / 1500: loss 5.083311\n",
      "iteration 1400 / 1500: loss 3.258157\n",
      "\n",
      "Evaluating combo:  (1e-05, 5000.0)\n",
      "iteration 0 / 1500: loss 157.596864\n",
      "iteration 100 / 1500: loss 4.530602\n",
      "iteration 200 / 1500: loss 3.604517\n",
      "iteration 300 / 1500: loss 3.388569\n",
      "iteration 400 / 1500: loss 3.382201\n",
      "iteration 500 / 1500: loss 4.574860\n",
      "iteration 600 / 1500: loss 4.443280\n",
      "iteration 700 / 1500: loss 5.840339\n",
      "iteration 800 / 1500: loss 3.371903\n",
      "iteration 900 / 1500: loss 4.843440\n",
      "iteration 1000 / 1500: loss 4.072692\n",
      "iteration 1100 / 1500: loss 3.847165\n",
      "iteration 1200 / 1500: loss 3.457970\n",
      "iteration 1300 / 1500: loss 3.575190\n",
      "iteration 1400 / 1500: loss 3.689531\n",
      "\n",
      "Evaluating combo:  (1e-05, 10000.0)\n",
      "iteration 0 / 1500: loss 315.150919\n",
      "iteration 100 / 1500: loss 4.939526\n",
      "iteration 200 / 1500: loss 6.021844\n",
      "iteration 300 / 1500: loss 3.761407\n",
      "iteration 400 / 1500: loss 5.143426\n",
      "iteration 500 / 1500: loss 5.030990\n",
      "iteration 600 / 1500: loss 4.746530\n",
      "iteration 700 / 1500: loss 4.051243\n",
      "iteration 800 / 1500: loss 5.758997\n",
      "iteration 900 / 1500: loss 4.454269\n",
      "iteration 1000 / 1500: loss 5.591129\n",
      "iteration 1100 / 1500: loss 5.338718\n",
      "iteration 1200 / 1500: loss 4.179659\n",
      "iteration 1300 / 1500: loss 4.131678\n",
      "iteration 1400 / 1500: loss 4.860327\n",
      "\n",
      "Evaluating combo:  (1e-05, 20000.0)\n",
      "iteration 0 / 1500: loss 629.029543\n",
      "iteration 100 / 1500: loss 7.816410\n",
      "iteration 200 / 1500: loss 6.171256\n",
      "iteration 300 / 1500: loss 6.688161\n",
      "iteration 400 / 1500: loss 9.442982\n",
      "iteration 500 / 1500: loss 6.884663\n",
      "iteration 600 / 1500: loss 8.637841\n",
      "iteration 700 / 1500: loss 7.606975\n",
      "iteration 800 / 1500: loss 6.354278\n",
      "iteration 900 / 1500: loss 7.373631\n",
      "iteration 1000 / 1500: loss 8.106838\n",
      "iteration 1100 / 1500: loss 5.951050\n",
      "iteration 1200 / 1500: loss 5.431022\n",
      "iteration 1300 / 1500: loss 5.540222\n",
      "iteration 1400 / 1500: loss 6.180102\n",
      "\n",
      "Evaluating combo:  (1e-05, 25000.0)\n",
      "iteration 0 / 1500: loss 777.713649\n",
      "iteration 100 / 1500: loss 8.293478\n",
      "iteration 200 / 1500: loss 8.788863\n",
      "iteration 300 / 1500: loss 4.924226\n",
      "iteration 400 / 1500: loss 8.018915\n",
      "iteration 500 / 1500: loss 7.598959\n",
      "iteration 600 / 1500: loss 7.069689\n",
      "iteration 700 / 1500: loss 6.024011\n",
      "iteration 800 / 1500: loss 6.435569\n",
      "iteration 900 / 1500: loss 6.603326\n",
      "iteration 1000 / 1500: loss 7.765352\n",
      "iteration 1100 / 1500: loss 8.941823\n",
      "iteration 1200 / 1500: loss 6.628315\n",
      "iteration 1300 / 1500: loss 8.400436\n",
      "iteration 1400 / 1500: loss 7.763450\n",
      "\n",
      "Evaluating combo:  (1e-05, 30000.0)\n",
      "iteration 0 / 1500: loss 921.667844\n",
      "iteration 100 / 1500: loss 8.664693\n",
      "iteration 200 / 1500: loss 7.464386\n",
      "iteration 300 / 1500: loss 8.160986\n",
      "iteration 400 / 1500: loss 7.843186\n",
      "iteration 500 / 1500: loss 8.394353\n",
      "iteration 600 / 1500: loss 8.671286\n",
      "iteration 700 / 1500: loss 10.359149\n",
      "iteration 800 / 1500: loss 10.305847\n",
      "iteration 900 / 1500: loss 8.949415\n",
      "iteration 1000 / 1500: loss 9.172857\n",
      "iteration 1100 / 1500: loss 8.282584\n",
      "iteration 1200 / 1500: loss 9.423897\n",
      "iteration 1300 / 1500: loss 10.779936\n",
      "iteration 1400 / 1500: loss 7.876885\n",
      "\n",
      "Evaluating combo:  (1e-05, 35000.0)\n",
      "iteration 0 / 1500: loss 1084.845752\n",
      "iteration 100 / 1500: loss 8.370529\n",
      "iteration 200 / 1500: loss 8.240265\n",
      "iteration 300 / 1500: loss 9.572769\n",
      "iteration 400 / 1500: loss 8.587002\n",
      "iteration 500 / 1500: loss 10.009011\n",
      "iteration 600 / 1500: loss 11.220270\n",
      "iteration 700 / 1500: loss 8.813019\n",
      "iteration 800 / 1500: loss 10.721932\n",
      "iteration 900 / 1500: loss 11.321721\n",
      "iteration 1000 / 1500: loss 10.226788\n",
      "iteration 1100 / 1500: loss 10.719608\n",
      "iteration 1200 / 1500: loss 10.057186\n",
      "iteration 1300 / 1500: loss 10.662880\n",
      "iteration 1400 / 1500: loss 10.694873\n",
      "\n",
      "Evaluating combo:  (1e-05, 40000.0)\n",
      "iteration 0 / 1500: loss 1240.929715\n",
      "iteration 100 / 1500: loss 11.638644\n",
      "iteration 200 / 1500: loss 11.248919\n",
      "iteration 300 / 1500: loss 10.549435\n",
      "iteration 400 / 1500: loss 11.109848\n",
      "iteration 500 / 1500: loss 12.956631\n",
      "iteration 600 / 1500: loss 10.662871\n",
      "iteration 700 / 1500: loss 10.504297\n",
      "iteration 800 / 1500: loss 13.001609\n",
      "iteration 900 / 1500: loss 10.496675\n",
      "iteration 1000 / 1500: loss 8.931147\n",
      "iteration 1100 / 1500: loss 10.113098\n",
      "iteration 1200 / 1500: loss 12.404828\n",
      "iteration 1300 / 1500: loss 9.613399\n",
      "iteration 1400 / 1500: loss 10.359830\n",
      "\n",
      "Evaluating combo:  (1e-05, 45000.0)\n",
      "iteration 0 / 1500: loss 1391.902457\n",
      "iteration 100 / 1500: loss 14.010433\n",
      "iteration 200 / 1500: loss 14.842373\n",
      "iteration 300 / 1500: loss 12.858864\n",
      "iteration 400 / 1500: loss 13.220150\n",
      "iteration 500 / 1500: loss 11.086859\n",
      "iteration 600 / 1500: loss 12.716579\n",
      "iteration 700 / 1500: loss 12.057295\n",
      "iteration 800 / 1500: loss 15.111040\n",
      "iteration 900 / 1500: loss 12.411214\n",
      "iteration 1000 / 1500: loss 12.598459\n",
      "iteration 1100 / 1500: loss 13.307675\n",
      "iteration 1200 / 1500: loss 13.497053\n",
      "iteration 1300 / 1500: loss 13.962518\n",
      "iteration 1400 / 1500: loss 16.116025\n",
      "\n",
      "Evaluating combo:  (1e-05, 50000.0)\n",
      "iteration 0 / 1500: loss 1507.374999\n",
      "iteration 100 / 1500: loss 14.708499\n",
      "iteration 200 / 1500: loss 14.475946\n",
      "iteration 300 / 1500: loss 17.134811\n",
      "iteration 400 / 1500: loss 16.532561\n",
      "iteration 500 / 1500: loss 15.922788\n",
      "iteration 600 / 1500: loss 13.969771\n",
      "iteration 700 / 1500: loss 14.356789\n",
      "iteration 800 / 1500: loss 15.159168\n",
      "iteration 900 / 1500: loss 16.009702\n",
      "iteration 1000 / 1500: loss 16.966607\n",
      "iteration 1100 / 1500: loss 17.996333\n",
      "iteration 1200 / 1500: loss 16.935379\n",
      "iteration 1300 / 1500: loss 15.049623\n",
      "iteration 1400 / 1500: loss 17.663615\n",
      "\n",
      "Evaluating combo:  (2e-05, 1000.0)\n",
      "iteration 0 / 1500: loss 36.158174\n",
      "iteration 100 / 1500: loss 4.424022\n",
      "iteration 200 / 1500: loss 8.447758\n",
      "iteration 300 / 1500: loss 11.581239\n",
      "iteration 400 / 1500: loss 9.549655\n",
      "iteration 500 / 1500: loss 10.179224\n",
      "iteration 600 / 1500: loss 7.287210\n",
      "iteration 700 / 1500: loss 13.673499\n",
      "iteration 800 / 1500: loss 8.811976\n",
      "iteration 900 / 1500: loss 8.357488\n",
      "iteration 1000 / 1500: loss 6.962406\n",
      "iteration 1100 / 1500: loss 8.595568\n",
      "iteration 1200 / 1500: loss 5.605272\n",
      "iteration 1300 / 1500: loss 8.188297\n",
      "iteration 1400 / 1500: loss 4.909165\n",
      "\n",
      "Evaluating combo:  (2e-05, 2000.0)\n",
      "iteration 0 / 1500: loss 68.028063\n",
      "iteration 100 / 1500: loss 9.269856\n",
      "iteration 200 / 1500: loss 13.468052\n",
      "iteration 300 / 1500: loss 8.227025\n",
      "iteration 400 / 1500: loss 8.045563\n",
      "iteration 500 / 1500: loss 6.858914\n",
      "iteration 600 / 1500: loss 9.257970\n",
      "iteration 700 / 1500: loss 7.637516\n",
      "iteration 800 / 1500: loss 8.050287\n",
      "iteration 900 / 1500: loss 8.352026\n",
      "iteration 1000 / 1500: loss 5.150182\n",
      "iteration 1100 / 1500: loss 7.137470\n",
      "iteration 1200 / 1500: loss 5.177402\n",
      "iteration 1300 / 1500: loss 8.101290\n",
      "iteration 1400 / 1500: loss 8.349933\n",
      "\n",
      "Evaluating combo:  (2e-05, 3000.0)\n",
      "iteration 0 / 1500: loss 97.037941\n",
      "iteration 100 / 1500: loss 7.982248\n",
      "iteration 200 / 1500: loss 10.292878\n",
      "iteration 300 / 1500: loss 9.753871\n",
      "iteration 400 / 1500: loss 7.646313\n",
      "iteration 500 / 1500: loss 8.886360\n",
      "iteration 600 / 1500: loss 10.067807\n",
      "iteration 700 / 1500: loss 12.109305\n",
      "iteration 800 / 1500: loss 6.490089\n",
      "iteration 900 / 1500: loss 10.180075\n",
      "iteration 1000 / 1500: loss 8.855249\n",
      "iteration 1100 / 1500: loss 13.030944\n",
      "iteration 1200 / 1500: loss 7.087559\n",
      "iteration 1300 / 1500: loss 6.715256\n",
      "iteration 1400 / 1500: loss 8.106045\n",
      "\n",
      "Evaluating combo:  (2e-05, 4000.0)\n",
      "iteration 0 / 1500: loss 127.730277\n",
      "iteration 100 / 1500: loss 7.098409\n",
      "iteration 200 / 1500: loss 11.155224\n",
      "iteration 300 / 1500: loss 9.330620\n",
      "iteration 400 / 1500: loss 10.686116\n",
      "iteration 500 / 1500: loss 6.418681\n",
      "iteration 600 / 1500: loss 8.555276\n",
      "iteration 700 / 1500: loss 9.856956\n",
      "iteration 800 / 1500: loss 7.978019\n",
      "iteration 900 / 1500: loss 12.398685\n",
      "iteration 1000 / 1500: loss 11.557833\n",
      "iteration 1100 / 1500: loss 7.960826\n",
      "iteration 1200 / 1500: loss 13.501536\n",
      "iteration 1300 / 1500: loss 6.421343\n",
      "iteration 1400 / 1500: loss 12.312102\n",
      "\n",
      "Evaluating combo:  (2e-05, 5000.0)\n",
      "iteration 0 / 1500: loss 158.406309\n",
      "iteration 100 / 1500: loss 7.853343\n",
      "iteration 200 / 1500: loss 10.656500\n",
      "iteration 300 / 1500: loss 12.233816\n",
      "iteration 400 / 1500: loss 6.900989\n",
      "iteration 500 / 1500: loss 10.112328\n",
      "iteration 600 / 1500: loss 11.510693\n",
      "iteration 700 / 1500: loss 12.547083\n",
      "iteration 800 / 1500: loss 10.276975\n",
      "iteration 900 / 1500: loss 8.503480\n",
      "iteration 1000 / 1500: loss 9.478631\n",
      "iteration 1100 / 1500: loss 13.329282\n",
      "iteration 1200 / 1500: loss 10.939994\n",
      "iteration 1300 / 1500: loss 9.459439\n",
      "iteration 1400 / 1500: loss 9.354951\n",
      "\n",
      "Evaluating combo:  (2e-05, 10000.0)\n",
      "iteration 0 / 1500: loss 311.808097\n",
      "iteration 100 / 1500: loss 19.318399\n",
      "iteration 200 / 1500: loss 11.777582\n",
      "iteration 300 / 1500: loss 14.203616\n",
      "iteration 400 / 1500: loss 13.009796\n",
      "iteration 500 / 1500: loss 11.035312\n",
      "iteration 600 / 1500: loss 14.233334\n",
      "iteration 700 / 1500: loss 14.793098\n",
      "iteration 800 / 1500: loss 14.777745\n",
      "iteration 900 / 1500: loss 10.096671\n",
      "iteration 1000 / 1500: loss 12.255231\n",
      "iteration 1100 / 1500: loss 13.918981\n",
      "iteration 1200 / 1500: loss 12.100996\n",
      "iteration 1300 / 1500: loss 18.540233\n",
      "iteration 1400 / 1500: loss 13.171924\n",
      "\n",
      "Evaluating combo:  (2e-05, 20000.0)\n",
      "iteration 0 / 1500: loss 618.369265\n",
      "iteration 100 / 1500: loss 23.686186\n",
      "iteration 200 / 1500: loss 25.734826\n",
      "iteration 300 / 1500: loss 27.082601\n",
      "iteration 400 / 1500: loss 30.513417\n",
      "iteration 500 / 1500: loss 24.123326\n",
      "iteration 600 / 1500: loss 25.426889\n",
      "iteration 700 / 1500: loss 23.871856\n",
      "iteration 800 / 1500: loss 21.665987\n",
      "iteration 900 / 1500: loss 28.364001\n",
      "iteration 1000 / 1500: loss 26.962313\n",
      "iteration 1100 / 1500: loss 20.935899\n",
      "iteration 1200 / 1500: loss 21.980927\n",
      "iteration 1300 / 1500: loss 23.571295\n",
      "iteration 1400 / 1500: loss 25.063390\n",
      "\n",
      "Evaluating combo:  (2e-05, 25000.0)\n",
      "iteration 0 / 1500: loss 776.391745\n",
      "iteration 100 / 1500: loss 34.706252\n",
      "iteration 200 / 1500: loss 38.130160\n",
      "iteration 300 / 1500: loss 26.901411\n",
      "iteration 400 / 1500: loss 35.355059\n",
      "iteration 500 / 1500: loss 30.360683\n",
      "iteration 600 / 1500: loss 40.511926\n",
      "iteration 700 / 1500: loss 30.229636\n",
      "iteration 800 / 1500: loss 37.326660\n",
      "iteration 900 / 1500: loss 37.176189\n",
      "iteration 1000 / 1500: loss 33.324536\n",
      "iteration 1100 / 1500: loss 30.456094\n",
      "iteration 1200 / 1500: loss 33.905438\n",
      "iteration 1300 / 1500: loss 28.838149\n",
      "iteration 1400 / 1500: loss 31.432943\n",
      "\n",
      "Evaluating combo:  (2e-05, 30000.0)\n",
      "iteration 0 / 1500: loss 916.779977\n",
      "iteration 100 / 1500: loss 49.376927\n",
      "iteration 200 / 1500: loss 53.306645\n",
      "iteration 300 / 1500: loss 56.317818\n",
      "iteration 400 / 1500: loss 54.484652\n",
      "iteration 500 / 1500: loss 42.045828\n",
      "iteration 600 / 1500: loss 48.868918\n",
      "iteration 700 / 1500: loss 50.442172\n",
      "iteration 800 / 1500: loss 55.679976\n",
      "iteration 900 / 1500: loss 48.762259\n",
      "iteration 1000 / 1500: loss 57.426576\n",
      "iteration 1100 / 1500: loss 55.447518\n",
      "iteration 1200 / 1500: loss 53.918456\n",
      "iteration 1300 / 1500: loss 50.849006\n",
      "iteration 1400 / 1500: loss 55.450958\n",
      "\n",
      "Evaluating combo:  (2e-05, 35000.0)\n",
      "iteration 0 / 1500: loss 1080.956142\n",
      "iteration 100 / 1500: loss 79.792863\n",
      "iteration 200 / 1500: loss 94.499027\n",
      "iteration 300 / 1500: loss 75.711718\n",
      "iteration 400 / 1500: loss 86.573113\n",
      "iteration 500 / 1500: loss 82.266881\n",
      "iteration 600 / 1500: loss 88.769109\n",
      "iteration 700 / 1500: loss 84.467967\n",
      "iteration 800 / 1500: loss 83.614468\n",
      "iteration 900 / 1500: loss 87.493788\n",
      "iteration 1000 / 1500: loss 81.149987\n",
      "iteration 1100 / 1500: loss 85.796403\n",
      "iteration 1200 / 1500: loss 83.358645\n",
      "iteration 1300 / 1500: loss 80.830937\n",
      "iteration 1400 / 1500: loss 89.593836\n",
      "\n",
      "Evaluating combo:  (2e-05, 40000.0)\n",
      "iteration 0 / 1500: loss 1235.390671\n",
      "iteration 100 / 1500: loss 162.840664\n",
      "iteration 200 / 1500: loss 165.775266\n",
      "iteration 300 / 1500: loss 195.707530\n",
      "iteration 400 / 1500: loss 175.549315\n",
      "iteration 500 / 1500: loss 173.140886\n",
      "iteration 600 / 1500: loss 187.387224\n",
      "iteration 700 / 1500: loss 176.835715\n",
      "iteration 800 / 1500: loss 161.485574\n",
      "iteration 900 / 1500: loss 165.592296\n",
      "iteration 1000 / 1500: loss 161.510114\n",
      "iteration 1100 / 1500: loss 180.607279\n",
      "iteration 1200 / 1500: loss 165.867866\n",
      "iteration 1300 / 1500: loss 170.161423\n",
      "iteration 1400 / 1500: loss 167.948183\n",
      "\n",
      "Evaluating combo:  (2e-05, 45000.0)\n",
      "iteration 0 / 1500: loss 1391.344873\n",
      "iteration 100 / 1500: loss 629.382592\n",
      "iteration 200 / 1500: loss 625.402792\n",
      "iteration 300 / 1500: loss 645.147421\n",
      "iteration 400 / 1500: loss 602.321562\n",
      "iteration 500 / 1500: loss 624.602136\n",
      "iteration 600 / 1500: loss 663.187953\n",
      "iteration 700 / 1500: loss 670.991872\n",
      "iteration 800 / 1500: loss 615.175389\n",
      "iteration 900 / 1500: loss 650.985001\n",
      "iteration 1000 / 1500: loss 583.264386\n",
      "iteration 1100 / 1500: loss 630.690577\n",
      "iteration 1200 / 1500: loss inf\n",
      "iteration 1300 / 1500: loss 630.615547\n",
      "iteration 1400 / 1500: loss 689.124780\n",
      "\n",
      "Evaluating combo:  (2e-05, 50000.0)\n",
      "iteration 0 / 1500: loss 1558.164875\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (3e-05, 1000.0)\n",
      "iteration 0 / 1500: loss 37.849291\n",
      "iteration 100 / 1500: loss 10.776271\n",
      "iteration 200 / 1500: loss 14.463544\n",
      "iteration 300 / 1500: loss 8.098268\n",
      "iteration 400 / 1500: loss 8.454892\n",
      "iteration 500 / 1500: loss 14.571532\n",
      "iteration 600 / 1500: loss 14.579130\n",
      "iteration 700 / 1500: loss 8.596083\n",
      "iteration 800 / 1500: loss 11.669186\n",
      "iteration 900 / 1500: loss 13.717276\n",
      "iteration 1000 / 1500: loss 12.461949\n",
      "iteration 1100 / 1500: loss 13.540052\n",
      "iteration 1200 / 1500: loss 17.029538\n",
      "iteration 1300 / 1500: loss 11.862247\n",
      "iteration 1400 / 1500: loss 14.317900\n",
      "\n",
      "Evaluating combo:  (3e-05, 2000.0)\n",
      "iteration 0 / 1500: loss 67.654522\n",
      "iteration 100 / 1500: loss 12.077210\n",
      "iteration 200 / 1500: loss 15.217905\n",
      "iteration 300 / 1500: loss 15.181742\n",
      "iteration 400 / 1500: loss 16.827268\n",
      "iteration 500 / 1500: loss 13.286260\n",
      "iteration 600 / 1500: loss 12.887664\n",
      "iteration 700 / 1500: loss 9.810301\n",
      "iteration 800 / 1500: loss 14.158524\n",
      "iteration 900 / 1500: loss 13.179770\n",
      "iteration 1000 / 1500: loss 16.569830\n",
      "iteration 1100 / 1500: loss 14.527034\n",
      "iteration 1200 / 1500: loss 12.531206\n",
      "iteration 1300 / 1500: loss 14.366979\n",
      "iteration 1400 / 1500: loss 9.455543\n",
      "\n",
      "Evaluating combo:  (3e-05, 3000.0)\n",
      "iteration 0 / 1500: loss 97.385489\n",
      "iteration 100 / 1500: loss 16.971513\n",
      "iteration 200 / 1500: loss 19.568416\n",
      "iteration 300 / 1500: loss 12.385482\n",
      "iteration 400 / 1500: loss 16.538999\n",
      "iteration 500 / 1500: loss 14.638712\n",
      "iteration 600 / 1500: loss 14.008902\n",
      "iteration 700 / 1500: loss 16.034968\n",
      "iteration 800 / 1500: loss 19.066708\n",
      "iteration 900 / 1500: loss 17.928241\n",
      "iteration 1000 / 1500: loss 13.198136\n",
      "iteration 1100 / 1500: loss 12.163034\n",
      "iteration 1200 / 1500: loss 15.168325\n",
      "iteration 1300 / 1500: loss 10.171275\n",
      "iteration 1400 / 1500: loss 12.281556\n",
      "\n",
      "Evaluating combo:  (3e-05, 4000.0)\n",
      "iteration 0 / 1500: loss 129.165304\n",
      "iteration 100 / 1500: loss 14.612781\n",
      "iteration 200 / 1500: loss 20.809837\n",
      "iteration 300 / 1500: loss 21.884680\n",
      "iteration 400 / 1500: loss 11.089902\n",
      "iteration 500 / 1500: loss 12.210810\n",
      "iteration 600 / 1500: loss 13.598402\n",
      "iteration 700 / 1500: loss 19.705066\n",
      "iteration 800 / 1500: loss 19.682080\n",
      "iteration 900 / 1500: loss 12.755369\n",
      "iteration 1000 / 1500: loss 14.919451\n",
      "iteration 1100 / 1500: loss 20.964895\n",
      "iteration 1200 / 1500: loss 14.347517\n",
      "iteration 1300 / 1500: loss 15.801375\n",
      "iteration 1400 / 1500: loss 15.152892\n",
      "\n",
      "Evaluating combo:  (3e-05, 5000.0)\n",
      "iteration 0 / 1500: loss 160.772830\n",
      "iteration 100 / 1500: loss 21.520572\n",
      "iteration 200 / 1500: loss 13.833968\n",
      "iteration 300 / 1500: loss 23.697597\n",
      "iteration 400 / 1500: loss 9.808020\n",
      "iteration 500 / 1500: loss 23.293504\n",
      "iteration 600 / 1500: loss 13.177547\n",
      "iteration 700 / 1500: loss 19.365261\n",
      "iteration 800 / 1500: loss 18.094350\n",
      "iteration 900 / 1500: loss 15.074437\n",
      "iteration 1000 / 1500: loss 13.303171\n",
      "iteration 1100 / 1500: loss 16.097597\n",
      "iteration 1200 / 1500: loss 17.440500\n",
      "iteration 1300 / 1500: loss 20.807131\n",
      "iteration 1400 / 1500: loss 15.923737\n",
      "\n",
      "Evaluating combo:  (3e-05, 10000.0)\n",
      "iteration 0 / 1500: loss 314.867333\n",
      "iteration 100 / 1500: loss 18.314129\n",
      "iteration 200 / 1500: loss 29.537321\n",
      "iteration 300 / 1500: loss 24.208888\n",
      "iteration 400 / 1500: loss 19.449950\n",
      "iteration 500 / 1500: loss 24.678730\n",
      "iteration 600 / 1500: loss 19.944975\n",
      "iteration 700 / 1500: loss 29.071334\n",
      "iteration 800 / 1500: loss 20.644812\n",
      "iteration 900 / 1500: loss 31.204718\n",
      "iteration 1000 / 1500: loss 24.430983\n",
      "iteration 1100 / 1500: loss 24.583679\n",
      "iteration 1200 / 1500: loss 33.269344\n",
      "iteration 1300 / 1500: loss 29.840580\n",
      "iteration 1400 / 1500: loss 17.550580\n",
      "\n",
      "Evaluating combo:  (3e-05, 20000.0)\n",
      "iteration 0 / 1500: loss 622.749851\n",
      "iteration 100 / 1500: loss 70.409638\n",
      "iteration 200 / 1500: loss 75.389555\n",
      "iteration 300 / 1500: loss 79.009447\n",
      "iteration 400 / 1500: loss 75.254319\n",
      "iteration 500 / 1500: loss 76.294955\n",
      "iteration 600 / 1500: loss 62.054623\n",
      "iteration 700 / 1500: loss 85.219035\n",
      "iteration 800 / 1500: loss 67.541686\n",
      "iteration 900 / 1500: loss 67.897803\n",
      "iteration 1000 / 1500: loss 75.906146\n",
      "iteration 1100 / 1500: loss 80.509535\n",
      "iteration 1200 / 1500: loss 79.692385\n",
      "iteration 1300 / 1500: loss 75.323799\n",
      "iteration 1400 / 1500: loss 74.210725\n",
      "\n",
      "Evaluating combo:  (3e-05, 25000.0)\n",
      "iteration 0 / 1500: loss 761.302440\n",
      "iteration 100 / 1500: loss 172.736079\n",
      "iteration 200 / 1500: loss 180.940767\n",
      "iteration 300 / 1500: loss 177.931185\n",
      "iteration 400 / 1500: loss 158.362523\n",
      "iteration 500 / 1500: loss 175.034914\n",
      "iteration 600 / 1500: loss 166.197293\n",
      "iteration 700 / 1500: loss 178.409569\n",
      "iteration 800 / 1500: loss 185.061109\n",
      "iteration 900 / 1500: loss 154.797590\n",
      "iteration 1000 / 1500: loss 156.363764\n",
      "iteration 1100 / 1500: loss 191.731032\n",
      "iteration 1200 / 1500: loss 184.858576\n",
      "iteration 1300 / 1500: loss 181.801409\n",
      "iteration 1400 / 1500: loss 165.844942\n",
      "\n",
      "Evaluating combo:  (3e-05, 30000.0)\n",
      "iteration 0 / 1500: loss 924.187297\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (3e-05, 35000.0)\n",
      "iteration 0 / 1500: loss 1093.569544\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (3e-05, 40000.0)\n",
      "iteration 0 / 1500: loss 1225.608939\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (3e-05, 45000.0)\n",
      "iteration 0 / 1500: loss 1403.199908\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (3e-05, 50000.0)\n",
      "iteration 0 / 1500: loss 1567.496717\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (4e-05, 1000.0)\n",
      "iteration 0 / 1500: loss 38.033156\n",
      "iteration 100 / 1500: loss 12.886602\n",
      "iteration 200 / 1500: loss 18.629870\n",
      "iteration 300 / 1500: loss 16.644243\n",
      "iteration 400 / 1500: loss 22.567136\n",
      "iteration 500 / 1500: loss 13.594037\n",
      "iteration 600 / 1500: loss 13.624900\n",
      "iteration 700 / 1500: loss 12.260938\n",
      "iteration 800 / 1500: loss 14.101896\n",
      "iteration 900 / 1500: loss 16.848770\n",
      "iteration 1000 / 1500: loss 16.654332\n",
      "iteration 1100 / 1500: loss 14.075338\n",
      "iteration 1200 / 1500: loss 18.840175\n",
      "iteration 1300 / 1500: loss 23.600468\n",
      "iteration 1400 / 1500: loss 13.311687\n",
      "\n",
      "Evaluating combo:  (4e-05, 2000.0)\n",
      "iteration 0 / 1500: loss 67.152801\n",
      "iteration 100 / 1500: loss 28.926995\n",
      "iteration 200 / 1500: loss 15.969281\n",
      "iteration 300 / 1500: loss 24.392776\n",
      "iteration 400 / 1500: loss 31.432442\n",
      "iteration 500 / 1500: loss 21.775169\n",
      "iteration 600 / 1500: loss 19.642067\n",
      "iteration 700 / 1500: loss 21.762965\n",
      "iteration 800 / 1500: loss 18.603448\n",
      "iteration 900 / 1500: loss 16.684359\n",
      "iteration 1000 / 1500: loss 14.296271\n",
      "iteration 1100 / 1500: loss 15.372496\n",
      "iteration 1200 / 1500: loss 22.540466\n",
      "iteration 1300 / 1500: loss 19.839768\n",
      "iteration 1400 / 1500: loss 20.145327\n",
      "\n",
      "Evaluating combo:  (4e-05, 3000.0)\n",
      "iteration 0 / 1500: loss 98.096864\n",
      "iteration 100 / 1500: loss 24.281631\n",
      "iteration 200 / 1500: loss 19.976339\n",
      "iteration 300 / 1500: loss 30.117754\n",
      "iteration 400 / 1500: loss 14.468981\n",
      "iteration 500 / 1500: loss 16.645416\n",
      "iteration 600 / 1500: loss 25.185255\n",
      "iteration 700 / 1500: loss 23.442672\n",
      "iteration 800 / 1500: loss 22.326203\n",
      "iteration 900 / 1500: loss 16.766656\n",
      "iteration 1000 / 1500: loss 17.911135\n",
      "iteration 1100 / 1500: loss 18.279883\n",
      "iteration 1200 / 1500: loss 19.977322\n",
      "iteration 1300 / 1500: loss 19.693691\n",
      "iteration 1400 / 1500: loss 20.871977\n",
      "\n",
      "Evaluating combo:  (4e-05, 4000.0)\n",
      "iteration 0 / 1500: loss 127.446469\n",
      "iteration 100 / 1500: loss 25.483041\n",
      "iteration 200 / 1500: loss 24.788129\n",
      "iteration 300 / 1500: loss 18.881066\n",
      "iteration 400 / 1500: loss 21.661497\n",
      "iteration 500 / 1500: loss 28.592953\n",
      "iteration 600 / 1500: loss 21.524282\n",
      "iteration 700 / 1500: loss 18.950015\n",
      "iteration 800 / 1500: loss 17.720947\n",
      "iteration 900 / 1500: loss 33.383426\n",
      "iteration 1000 / 1500: loss 22.933226\n",
      "iteration 1100 / 1500: loss 21.081049\n",
      "iteration 1200 / 1500: loss 24.707845\n",
      "iteration 1300 / 1500: loss 11.644571\n",
      "iteration 1400 / 1500: loss 18.364329\n",
      "\n",
      "Evaluating combo:  (4e-05, 5000.0)\n",
      "iteration 0 / 1500: loss 160.177456\n",
      "iteration 100 / 1500: loss 31.004108\n",
      "iteration 200 / 1500: loss 31.259057\n",
      "iteration 300 / 1500: loss 31.164299\n",
      "iteration 400 / 1500: loss 32.339615\n",
      "iteration 500 / 1500: loss 26.148228\n",
      "iteration 600 / 1500: loss 25.574049\n",
      "iteration 700 / 1500: loss 32.461828\n",
      "iteration 800 / 1500: loss 24.849026\n",
      "iteration 900 / 1500: loss 22.447289\n",
      "iteration 1000 / 1500: loss 28.515426\n",
      "iteration 1100 / 1500: loss 37.547123\n",
      "iteration 1200 / 1500: loss 30.070863\n",
      "iteration 1300 / 1500: loss 28.312132\n",
      "iteration 1400 / 1500: loss 40.474625\n",
      "\n",
      "Evaluating combo:  (4e-05, 10000.0)\n",
      "iteration 0 / 1500: loss 312.406332\n",
      "iteration 100 / 1500: loss 54.024063\n",
      "iteration 200 / 1500: loss 46.243814\n",
      "iteration 300 / 1500: loss 42.701475\n",
      "iteration 400 / 1500: loss 43.724718\n",
      "iteration 500 / 1500: loss 52.264070\n",
      "iteration 600 / 1500: loss 54.194496\n",
      "iteration 700 / 1500: loss 53.291277\n",
      "iteration 800 / 1500: loss 50.288164\n",
      "iteration 900 / 1500: loss 46.999115\n",
      "iteration 1000 / 1500: loss 46.506558\n",
      "iteration 1100 / 1500: loss 50.568796\n",
      "iteration 1200 / 1500: loss 45.818165\n",
      "iteration 1300 / 1500: loss 38.332676\n",
      "iteration 1400 / 1500: loss 50.594631\n",
      "\n",
      "Evaluating combo:  (4e-05, 20000.0)\n",
      "iteration 0 / 1500: loss 613.368213\n",
      "iteration 100 / 1500: loss 384.592516\n",
      "iteration 200 / 1500: loss 342.107042\n",
      "iteration 300 / 1500: loss 342.887386\n",
      "iteration 400 / 1500: loss 349.065172\n",
      "iteration 500 / 1500: loss 359.680843\n",
      "iteration 600 / 1500: loss 377.598380\n",
      "iteration 700 / 1500: loss 346.062083\n",
      "iteration 800 / 1500: loss 364.065500\n",
      "iteration 900 / 1500: loss 359.941278\n",
      "iteration 1000 / 1500: loss 354.626343\n",
      "iteration 1100 / 1500: loss 342.992706\n",
      "iteration 1200 / 1500: loss 369.187754\n",
      "iteration 1300 / 1500: loss 322.497816\n",
      "iteration 1400 / 1500: loss 353.944243\n",
      "\n",
      "Evaluating combo:  (4e-05, 25000.0)\n",
      "iteration 0 / 1500: loss 782.671832\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (4e-05, 30000.0)\n",
      "iteration 0 / 1500: loss 929.467024\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (4e-05, 35000.0)\n",
      "iteration 0 / 1500: loss 1078.112981\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (4e-05, 40000.0)\n",
      "iteration 0 / 1500: loss 1210.765875\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (4e-05, 45000.0)\n",
      "iteration 0 / 1500: loss 1370.300447\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (4e-05, 50000.0)\n",
      "iteration 0 / 1500: loss 1530.609709\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (5e-05, 1000.0)\n",
      "iteration 0 / 1500: loss 36.640467\n",
      "iteration 100 / 1500: loss 12.744785\n",
      "iteration 200 / 1500: loss 14.967777\n",
      "iteration 300 / 1500: loss 25.038179\n",
      "iteration 400 / 1500: loss 18.647952\n",
      "iteration 500 / 1500: loss 19.881496\n",
      "iteration 600 / 1500: loss 24.230269\n",
      "iteration 700 / 1500: loss 20.002007\n",
      "iteration 800 / 1500: loss 22.097166\n",
      "iteration 900 / 1500: loss 23.967331\n",
      "iteration 1000 / 1500: loss 17.002142\n",
      "iteration 1100 / 1500: loss 20.484639\n",
      "iteration 1200 / 1500: loss 21.729720\n",
      "iteration 1300 / 1500: loss 21.727290\n",
      "iteration 1400 / 1500: loss 28.137738\n",
      "\n",
      "Evaluating combo:  (5e-05, 2000.0)\n",
      "iteration 0 / 1500: loss 66.662990\n",
      "iteration 100 / 1500: loss 12.937322\n",
      "iteration 200 / 1500: loss 23.906545\n",
      "iteration 300 / 1500: loss 24.363709\n",
      "iteration 400 / 1500: loss 36.206079\n",
      "iteration 500 / 1500: loss 20.945990\n",
      "iteration 600 / 1500: loss 25.799754\n",
      "iteration 700 / 1500: loss 28.901767\n",
      "iteration 800 / 1500: loss 18.920936\n",
      "iteration 900 / 1500: loss 26.070997\n",
      "iteration 1000 / 1500: loss 34.101762\n",
      "iteration 1100 / 1500: loss 25.020926\n",
      "iteration 1200 / 1500: loss 28.160260\n",
      "iteration 1300 / 1500: loss 24.917717\n",
      "iteration 1400 / 1500: loss 27.479687\n",
      "\n",
      "Evaluating combo:  (5e-05, 3000.0)\n",
      "iteration 0 / 1500: loss 96.378366\n",
      "iteration 100 / 1500: loss 24.636997\n",
      "iteration 200 / 1500: loss 21.418429\n",
      "iteration 300 / 1500: loss 39.212672\n",
      "iteration 400 / 1500: loss 17.390500\n",
      "iteration 500 / 1500: loss 31.422261\n",
      "iteration 600 / 1500: loss 28.335128\n",
      "iteration 700 / 1500: loss 18.148328\n",
      "iteration 800 / 1500: loss 19.894476\n",
      "iteration 900 / 1500: loss 27.082162\n",
      "iteration 1000 / 1500: loss 26.437291\n",
      "iteration 1100 / 1500: loss 33.889789\n",
      "iteration 1200 / 1500: loss 42.865357\n",
      "iteration 1300 / 1500: loss 36.077700\n",
      "iteration 1400 / 1500: loss 24.776519\n",
      "\n",
      "Evaluating combo:  (5e-05, 4000.0)\n",
      "iteration 0 / 1500: loss 129.073140\n",
      "iteration 100 / 1500: loss 39.523265\n",
      "iteration 200 / 1500: loss 31.917652\n",
      "iteration 300 / 1500: loss 32.800001\n",
      "iteration 400 / 1500: loss 24.458337\n",
      "iteration 500 / 1500: loss 24.984720\n",
      "iteration 600 / 1500: loss 20.112083\n",
      "iteration 700 / 1500: loss 38.234877\n",
      "iteration 800 / 1500: loss 37.853395\n",
      "iteration 900 / 1500: loss 26.691482\n",
      "iteration 1000 / 1500: loss 38.481606\n",
      "iteration 1100 / 1500: loss 37.542619\n",
      "iteration 1200 / 1500: loss 41.249330\n",
      "iteration 1300 / 1500: loss 24.950165\n",
      "iteration 1400 / 1500: loss 55.643927\n",
      "\n",
      "Evaluating combo:  (5e-05, 5000.0)\n",
      "iteration 0 / 1500: loss 159.216268\n",
      "iteration 100 / 1500: loss 32.888640\n",
      "iteration 200 / 1500: loss 55.204490\n",
      "iteration 300 / 1500: loss 54.689221\n",
      "iteration 400 / 1500: loss 30.677496\n",
      "iteration 500 / 1500: loss 44.572383\n",
      "iteration 600 / 1500: loss 41.384768\n",
      "iteration 700 / 1500: loss 40.652061\n",
      "iteration 800 / 1500: loss 30.331756\n",
      "iteration 900 / 1500: loss 42.722713\n",
      "iteration 1000 / 1500: loss 38.793533\n",
      "iteration 1100 / 1500: loss 38.199692\n",
      "iteration 1200 / 1500: loss 43.742321\n",
      "iteration 1300 / 1500: loss 48.218690\n",
      "iteration 1400 / 1500: loss 42.504116\n",
      "\n",
      "Evaluating combo:  (5e-05, 10000.0)\n",
      "iteration 0 / 1500: loss 310.973019\n",
      "iteration 100 / 1500: loss 81.632546\n",
      "iteration 200 / 1500: loss 89.051467\n",
      "iteration 300 / 1500: loss 82.403614\n",
      "iteration 400 / 1500: loss 78.116845\n",
      "iteration 500 / 1500: loss 70.385745\n",
      "iteration 600 / 1500: loss 93.599356\n",
      "iteration 700 / 1500: loss 69.057199\n",
      "iteration 800 / 1500: loss 75.360539\n",
      "iteration 900 / 1500: loss 77.191657\n",
      "iteration 1000 / 1500: loss 78.120982\n",
      "iteration 1100 / 1500: loss 83.881780\n",
      "iteration 1200 / 1500: loss 80.944666\n",
      "iteration 1300 / 1500: loss 77.588661\n",
      "iteration 1400 / 1500: loss 76.950766\n",
      "\n",
      "Evaluating combo:  (5e-05, 20000.0)\n",
      "iteration 0 / 1500: loss 614.327044\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (5e-05, 25000.0)\n",
      "iteration 0 / 1500: loss 769.122014\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (5e-05, 30000.0)\n",
      "iteration 0 / 1500: loss 929.771080\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (5e-05, 35000.0)\n",
      "iteration 0 / 1500: loss 1072.381537\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (5e-05, 40000.0)\n",
      "iteration 0 / 1500: loss 1235.410384\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (5e-05, 45000.0)\n",
      "iteration 0 / 1500: loss 1390.444790\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "\n",
      "Evaluating combo:  (5e-05, 50000.0)\n",
      "iteration 0 / 1500: loss 1551.554602\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "lr 1.000000e-07 reg 1.000000e+03 train accuracy: 0.262959 val accuracy: 0.280000\n",
      "lr 1.000000e-07 reg 2.000000e+03 train accuracy: 0.286286 val accuracy: 0.306000\n",
      "lr 1.000000e-07 reg 3.000000e+03 train accuracy: 0.298694 val accuracy: 0.310000\n",
      "lr 1.000000e-07 reg 4.000000e+03 train accuracy: 0.320388 val accuracy: 0.361000\n",
      "lr 1.000000e-07 reg 5.000000e+03 train accuracy: 0.332959 val accuracy: 0.345000\n",
      "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.354408 val accuracy: 0.356000\n",
      "lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.333776 val accuracy: 0.346000\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.332490 val accuracy: 0.354000\n",
      "lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.327490 val accuracy: 0.346000\n",
      "lr 1.000000e-07 reg 3.500000e+04 train accuracy: 0.325102 val accuracy: 0.342000\n",
      "lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.320041 val accuracy: 0.332000\n",
      "lr 1.000000e-07 reg 4.500000e+04 train accuracy: 0.300122 val accuracy: 0.318000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.309429 val accuracy: 0.321000\n",
      "lr 2.000000e-07 reg 1.000000e+03 train accuracy: 0.316510 val accuracy: 0.336000\n",
      "lr 2.000000e-07 reg 2.000000e+03 train accuracy: 0.351429 val accuracy: 0.358000\n",
      "lr 2.000000e-07 reg 3.000000e+03 train accuracy: 0.365122 val accuracy: 0.374000\n",
      "lr 2.000000e-07 reg 4.000000e+03 train accuracy: 0.371653 val accuracy: 0.385000\n",
      "lr 2.000000e-07 reg 5.000000e+03 train accuracy: 0.372918 val accuracy: 0.382000\n",
      "lr 2.000000e-07 reg 1.000000e+04 train accuracy: 0.355673 val accuracy: 0.369000\n",
      "lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.338510 val accuracy: 0.351000\n",
      "lr 2.000000e-07 reg 2.500000e+04 train accuracy: 0.322490 val accuracy: 0.343000\n",
      "lr 2.000000e-07 reg 3.000000e+04 train accuracy: 0.325673 val accuracy: 0.341000\n",
      "lr 2.000000e-07 reg 3.500000e+04 train accuracy: 0.309551 val accuracy: 0.327000\n",
      "lr 2.000000e-07 reg 4.000000e+04 train accuracy: 0.306020 val accuracy: 0.316000\n",
      "lr 2.000000e-07 reg 4.500000e+04 train accuracy: 0.310163 val accuracy: 0.320000\n",
      "lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.310959 val accuracy: 0.317000\n",
      "lr 3.000000e-07 reg 1.000000e+03 train accuracy: 0.350408 val accuracy: 0.375000\n",
      "lr 3.000000e-07 reg 2.000000e+03 train accuracy: 0.376837 val accuracy: 0.375000\n",
      "lr 3.000000e-07 reg 3.000000e+03 train accuracy: 0.384755 val accuracy: 0.394000\n",
      "lr 3.000000e-07 reg 4.000000e+03 train accuracy: 0.373510 val accuracy: 0.384000\n",
      "lr 3.000000e-07 reg 5.000000e+03 train accuracy: 0.375286 val accuracy: 0.378000\n",
      "lr 3.000000e-07 reg 1.000000e+04 train accuracy: 0.355265 val accuracy: 0.373000\n",
      "lr 3.000000e-07 reg 2.000000e+04 train accuracy: 0.338592 val accuracy: 0.348000\n",
      "lr 3.000000e-07 reg 2.500000e+04 train accuracy: 0.329490 val accuracy: 0.338000\n",
      "lr 3.000000e-07 reg 3.000000e+04 train accuracy: 0.324469 val accuracy: 0.336000\n",
      "lr 3.000000e-07 reg 3.500000e+04 train accuracy: 0.313510 val accuracy: 0.334000\n",
      "lr 3.000000e-07 reg 4.000000e+04 train accuracy: 0.315776 val accuracy: 0.327000\n",
      "lr 3.000000e-07 reg 4.500000e+04 train accuracy: 0.304102 val accuracy: 0.315000\n",
      "lr 3.000000e-07 reg 5.000000e+04 train accuracy: 0.308265 val accuracy: 0.310000\n",
      "lr 4.000000e-07 reg 1.000000e+03 train accuracy: 0.371490 val accuracy: 0.381000\n",
      "lr 4.000000e-07 reg 2.000000e+03 train accuracy: 0.386735 val accuracy: 0.388000\n",
      "lr 4.000000e-07 reg 3.000000e+03 train accuracy: 0.386490 val accuracy: 0.389000\n",
      "lr 4.000000e-07 reg 4.000000e+03 train accuracy: 0.377184 val accuracy: 0.385000\n",
      "lr 4.000000e-07 reg 5.000000e+03 train accuracy: 0.365918 val accuracy: 0.378000\n",
      "lr 4.000000e-07 reg 1.000000e+04 train accuracy: 0.353735 val accuracy: 0.347000\n",
      "lr 4.000000e-07 reg 2.000000e+04 train accuracy: 0.332449 val accuracy: 0.341000\n",
      "lr 4.000000e-07 reg 2.500000e+04 train accuracy: 0.320878 val accuracy: 0.339000\n",
      "lr 4.000000e-07 reg 3.000000e+04 train accuracy: 0.312061 val accuracy: 0.330000\n",
      "lr 4.000000e-07 reg 3.500000e+04 train accuracy: 0.317408 val accuracy: 0.332000\n",
      "lr 4.000000e-07 reg 4.000000e+04 train accuracy: 0.310959 val accuracy: 0.331000\n",
      "lr 4.000000e-07 reg 4.500000e+04 train accuracy: 0.295245 val accuracy: 0.306000\n",
      "lr 4.000000e-07 reg 5.000000e+04 train accuracy: 0.301265 val accuracy: 0.306000\n",
      "lr 5.000000e-07 reg 1.000000e+03 train accuracy: 0.388653 val accuracy: 0.398000\n",
      "lr 5.000000e-07 reg 2.000000e+03 train accuracy: 0.391980 val accuracy: 0.393000\n",
      "lr 5.000000e-07 reg 3.000000e+03 train accuracy: 0.386531 val accuracy: 0.388000\n",
      "lr 5.000000e-07 reg 4.000000e+03 train accuracy: 0.377082 val accuracy: 0.386000\n",
      "lr 5.000000e-07 reg 5.000000e+03 train accuracy: 0.372429 val accuracy: 0.380000\n",
      "lr 5.000000e-07 reg 1.000000e+04 train accuracy: 0.356122 val accuracy: 0.366000\n",
      "lr 5.000000e-07 reg 2.000000e+04 train accuracy: 0.321551 val accuracy: 0.332000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.329367 val accuracy: 0.342000\n",
      "lr 5.000000e-07 reg 3.000000e+04 train accuracy: 0.308163 val accuracy: 0.327000\n",
      "lr 5.000000e-07 reg 3.500000e+04 train accuracy: 0.319469 val accuracy: 0.318000\n",
      "lr 5.000000e-07 reg 4.000000e+04 train accuracy: 0.314735 val accuracy: 0.332000\n",
      "lr 5.000000e-07 reg 4.500000e+04 train accuracy: 0.306694 val accuracy: 0.325000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.302429 val accuracy: 0.307000\n",
      "lr 1.000000e-06 reg 1.000000e+03 train accuracy: 0.397184 val accuracy: 0.404000\n",
      "lr 1.000000e-06 reg 2.000000e+03 train accuracy: 0.388224 val accuracy: 0.397000\n",
      "lr 1.000000e-06 reg 3.000000e+03 train accuracy: 0.381449 val accuracy: 0.390000\n",
      "lr 1.000000e-06 reg 4.000000e+03 train accuracy: 0.373000 val accuracy: 0.375000\n",
      "lr 1.000000e-06 reg 5.000000e+03 train accuracy: 0.367204 val accuracy: 0.376000\n",
      "lr 1.000000e-06 reg 1.000000e+04 train accuracy: 0.340102 val accuracy: 0.362000\n",
      "lr 1.000000e-06 reg 2.000000e+04 train accuracy: 0.319429 val accuracy: 0.333000\n",
      "lr 1.000000e-06 reg 2.500000e+04 train accuracy: 0.326449 val accuracy: 0.326000\n",
      "lr 1.000000e-06 reg 3.000000e+04 train accuracy: 0.316367 val accuracy: 0.325000\n",
      "lr 1.000000e-06 reg 3.500000e+04 train accuracy: 0.306633 val accuracy: 0.313000\n",
      "lr 1.000000e-06 reg 4.000000e+04 train accuracy: 0.312633 val accuracy: 0.311000\n",
      "lr 1.000000e-06 reg 4.500000e+04 train accuracy: 0.303490 val accuracy: 0.318000\n",
      "lr 1.000000e-06 reg 5.000000e+04 train accuracy: 0.290653 val accuracy: 0.302000\n",
      "lr 2.000000e-06 reg 1.000000e+03 train accuracy: 0.395714 val accuracy: 0.382000\n",
      "lr 2.000000e-06 reg 2.000000e+03 train accuracy: 0.382204 val accuracy: 0.391000\n",
      "lr 2.000000e-06 reg 3.000000e+03 train accuracy: 0.367224 val accuracy: 0.375000\n",
      "lr 2.000000e-06 reg 4.000000e+03 train accuracy: 0.365367 val accuracy: 0.368000\n",
      "lr 2.000000e-06 reg 5.000000e+03 train accuracy: 0.360816 val accuracy: 0.365000\n",
      "lr 2.000000e-06 reg 1.000000e+04 train accuracy: 0.331245 val accuracy: 0.350000\n",
      "lr 2.000000e-06 reg 2.000000e+04 train accuracy: 0.316265 val accuracy: 0.326000\n",
      "lr 2.000000e-06 reg 2.500000e+04 train accuracy: 0.309959 val accuracy: 0.314000\n",
      "lr 2.000000e-06 reg 3.000000e+04 train accuracy: 0.314918 val accuracy: 0.322000\n",
      "lr 2.000000e-06 reg 3.500000e+04 train accuracy: 0.290673 val accuracy: 0.296000\n",
      "lr 2.000000e-06 reg 4.000000e+04 train accuracy: 0.289796 val accuracy: 0.280000\n",
      "lr 2.000000e-06 reg 4.500000e+04 train accuracy: 0.290510 val accuracy: 0.305000\n",
      "lr 2.000000e-06 reg 5.000000e+04 train accuracy: 0.278122 val accuracy: 0.289000\n",
      "lr 3.000000e-06 reg 1.000000e+03 train accuracy: 0.376878 val accuracy: 0.372000\n",
      "lr 3.000000e-06 reg 2.000000e+03 train accuracy: 0.345204 val accuracy: 0.361000\n",
      "lr 3.000000e-06 reg 3.000000e+03 train accuracy: 0.350653 val accuracy: 0.370000\n",
      "lr 3.000000e-06 reg 4.000000e+03 train accuracy: 0.359286 val accuracy: 0.344000\n",
      "lr 3.000000e-06 reg 5.000000e+03 train accuracy: 0.349776 val accuracy: 0.350000\n",
      "lr 3.000000e-06 reg 1.000000e+04 train accuracy: 0.328020 val accuracy: 0.317000\n",
      "lr 3.000000e-06 reg 2.000000e+04 train accuracy: 0.316959 val accuracy: 0.317000\n",
      "lr 3.000000e-06 reg 2.500000e+04 train accuracy: 0.308939 val accuracy: 0.336000\n",
      "lr 3.000000e-06 reg 3.000000e+04 train accuracy: 0.232755 val accuracy: 0.242000\n",
      "lr 3.000000e-06 reg 3.500000e+04 train accuracy: 0.265816 val accuracy: 0.277000\n",
      "lr 3.000000e-06 reg 4.000000e+04 train accuracy: 0.287245 val accuracy: 0.302000\n",
      "lr 3.000000e-06 reg 4.500000e+04 train accuracy: 0.249531 val accuracy: 0.250000\n",
      "lr 3.000000e-06 reg 5.000000e+04 train accuracy: 0.266531 val accuracy: 0.271000\n",
      "lr 4.000000e-06 reg 1.000000e+03 train accuracy: 0.347102 val accuracy: 0.362000\n",
      "lr 4.000000e-06 reg 2.000000e+03 train accuracy: 0.326571 val accuracy: 0.339000\n",
      "lr 4.000000e-06 reg 3.000000e+03 train accuracy: 0.328633 val accuracy: 0.337000\n",
      "lr 4.000000e-06 reg 4.000000e+03 train accuracy: 0.329347 val accuracy: 0.343000\n",
      "lr 4.000000e-06 reg 5.000000e+03 train accuracy: 0.338531 val accuracy: 0.335000\n",
      "lr 4.000000e-06 reg 1.000000e+04 train accuracy: 0.300245 val accuracy: 0.332000\n",
      "lr 4.000000e-06 reg 2.000000e+04 train accuracy: 0.288429 val accuracy: 0.305000\n",
      "lr 4.000000e-06 reg 2.500000e+04 train accuracy: 0.277388 val accuracy: 0.295000\n",
      "lr 4.000000e-06 reg 3.000000e+04 train accuracy: 0.277286 val accuracy: 0.292000\n",
      "lr 4.000000e-06 reg 3.500000e+04 train accuracy: 0.255245 val accuracy: 0.252000\n",
      "lr 4.000000e-06 reg 4.000000e+04 train accuracy: 0.216286 val accuracy: 0.218000\n",
      "lr 4.000000e-06 reg 4.500000e+04 train accuracy: 0.164796 val accuracy: 0.172000\n",
      "lr 4.000000e-06 reg 5.000000e+04 train accuracy: 0.206551 val accuracy: 0.198000\n",
      "lr 5.000000e-06 reg 1.000000e+03 train accuracy: 0.340224 val accuracy: 0.347000\n",
      "lr 5.000000e-06 reg 2.000000e+03 train accuracy: 0.346531 val accuracy: 0.351000\n",
      "lr 5.000000e-06 reg 3.000000e+03 train accuracy: 0.324122 val accuracy: 0.337000\n",
      "lr 5.000000e-06 reg 4.000000e+03 train accuracy: 0.305204 val accuracy: 0.321000\n",
      "lr 5.000000e-06 reg 5.000000e+03 train accuracy: 0.288429 val accuracy: 0.287000\n",
      "lr 5.000000e-06 reg 1.000000e+04 train accuracy: 0.270265 val accuracy: 0.277000\n",
      "lr 5.000000e-06 reg 2.000000e+04 train accuracy: 0.289020 val accuracy: 0.293000\n",
      "lr 5.000000e-06 reg 2.500000e+04 train accuracy: 0.221367 val accuracy: 0.225000\n",
      "lr 5.000000e-06 reg 3.000000e+04 train accuracy: 0.177286 val accuracy: 0.176000\n",
      "lr 5.000000e-06 reg 3.500000e+04 train accuracy: 0.142347 val accuracy: 0.133000\n",
      "lr 5.000000e-06 reg 4.000000e+04 train accuracy: 0.148408 val accuracy: 0.152000\n",
      "lr 5.000000e-06 reg 4.500000e+04 train accuracy: 0.131347 val accuracy: 0.129000\n",
      "lr 5.000000e-06 reg 5.000000e+04 train accuracy: 0.137571 val accuracy: 0.126000\n",
      "lr 1.000000e-05 reg 1.000000e+03 train accuracy: 0.224796 val accuracy: 0.222000\n",
      "lr 1.000000e-05 reg 2.000000e+03 train accuracy: 0.211306 val accuracy: 0.227000\n",
      "lr 1.000000e-05 reg 3.000000e+03 train accuracy: 0.197959 val accuracy: 0.208000\n",
      "lr 1.000000e-05 reg 4.000000e+03 train accuracy: 0.192816 val accuracy: 0.181000\n",
      "lr 1.000000e-05 reg 5.000000e+03 train accuracy: 0.228122 val accuracy: 0.226000\n",
      "lr 1.000000e-05 reg 1.000000e+04 train accuracy: 0.171000 val accuracy: 0.182000\n",
      "lr 1.000000e-05 reg 2.000000e+04 train accuracy: 0.160000 val accuracy: 0.153000\n",
      "lr 1.000000e-05 reg 2.500000e+04 train accuracy: 0.171898 val accuracy: 0.186000\n",
      "lr 1.000000e-05 reg 3.000000e+04 train accuracy: 0.146918 val accuracy: 0.145000\n",
      "lr 1.000000e-05 reg 3.500000e+04 train accuracy: 0.116469 val accuracy: 0.102000\n",
      "lr 1.000000e-05 reg 4.000000e+04 train accuracy: 0.083224 val accuracy: 0.078000\n",
      "lr 1.000000e-05 reg 4.500000e+04 train accuracy: 0.100735 val accuracy: 0.106000\n",
      "lr 1.000000e-05 reg 5.000000e+04 train accuracy: 0.070959 val accuracy: 0.063000\n",
      "lr 2.000000e-05 reg 1.000000e+03 train accuracy: 0.175286 val accuracy: 0.171000\n",
      "lr 2.000000e-05 reg 2.000000e+03 train accuracy: 0.183796 val accuracy: 0.194000\n",
      "lr 2.000000e-05 reg 3.000000e+03 train accuracy: 0.160898 val accuracy: 0.166000\n",
      "lr 2.000000e-05 reg 4.000000e+03 train accuracy: 0.171694 val accuracy: 0.176000\n",
      "lr 2.000000e-05 reg 5.000000e+03 train accuracy: 0.187245 val accuracy: 0.173000\n",
      "lr 2.000000e-05 reg 1.000000e+04 train accuracy: 0.124000 val accuracy: 0.100000\n",
      "lr 2.000000e-05 reg 2.000000e+04 train accuracy: 0.105082 val accuracy: 0.100000\n",
      "lr 2.000000e-05 reg 2.500000e+04 train accuracy: 0.115878 val accuracy: 0.122000\n",
      "lr 2.000000e-05 reg 3.000000e+04 train accuracy: 0.080918 val accuracy: 0.072000\n",
      "lr 2.000000e-05 reg 3.500000e+04 train accuracy: 0.103776 val accuracy: 0.101000\n",
      "lr 2.000000e-05 reg 4.000000e+04 train accuracy: 0.062714 val accuracy: 0.064000\n",
      "lr 2.000000e-05 reg 4.500000e+04 train accuracy: 0.092347 val accuracy: 0.106000\n",
      "lr 2.000000e-05 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.000000e-05 reg 1.000000e+03 train accuracy: 0.220388 val accuracy: 0.240000\n",
      "lr 3.000000e-05 reg 2.000000e+03 train accuracy: 0.160714 val accuracy: 0.168000\n",
      "lr 3.000000e-05 reg 3.000000e+03 train accuracy: 0.195388 val accuracy: 0.211000\n",
      "lr 3.000000e-05 reg 4.000000e+03 train accuracy: 0.123490 val accuracy: 0.122000\n",
      "lr 3.000000e-05 reg 5.000000e+03 train accuracy: 0.158755 val accuracy: 0.163000\n",
      "lr 3.000000e-05 reg 1.000000e+04 train accuracy: 0.121878 val accuracy: 0.119000\n",
      "lr 3.000000e-05 reg 2.000000e+04 train accuracy: 0.101673 val accuracy: 0.083000\n",
      "lr 3.000000e-05 reg 2.500000e+04 train accuracy: 0.061061 val accuracy: 0.058000\n",
      "lr 3.000000e-05 reg 3.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.000000e-05 reg 3.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.000000e-05 reg 4.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.000000e-05 reg 4.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.000000e-05 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.000000e-05 reg 1.000000e+03 train accuracy: 0.143510 val accuracy: 0.165000\n",
      "lr 4.000000e-05 reg 2.000000e+03 train accuracy: 0.147980 val accuracy: 0.154000\n",
      "lr 4.000000e-05 reg 3.000000e+03 train accuracy: 0.181939 val accuracy: 0.179000\n",
      "lr 4.000000e-05 reg 4.000000e+03 train accuracy: 0.143122 val accuracy: 0.142000\n",
      "lr 4.000000e-05 reg 5.000000e+03 train accuracy: 0.134735 val accuracy: 0.125000\n",
      "lr 4.000000e-05 reg 1.000000e+04 train accuracy: 0.100143 val accuracy: 0.089000\n",
      "lr 4.000000e-05 reg 2.000000e+04 train accuracy: 0.096163 val accuracy: 0.097000\n",
      "lr 4.000000e-05 reg 2.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.000000e-05 reg 3.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.000000e-05 reg 3.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.000000e-05 reg 4.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.000000e-05 reg 4.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.000000e-05 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 1.000000e+03 train accuracy: 0.164020 val accuracy: 0.175000\n",
      "lr 5.000000e-05 reg 2.000000e+03 train accuracy: 0.151878 val accuracy: 0.156000\n",
      "lr 5.000000e-05 reg 3.000000e+03 train accuracy: 0.160184 val accuracy: 0.138000\n",
      "lr 5.000000e-05 reg 4.000000e+03 train accuracy: 0.186163 val accuracy: 0.187000\n",
      "lr 5.000000e-05 reg 5.000000e+03 train accuracy: 0.133776 val accuracy: 0.154000\n",
      "lr 5.000000e-05 reg 1.000000e+04 train accuracy: 0.082020 val accuracy: 0.082000\n",
      "lr 5.000000e-05 reg 2.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 2.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 3.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 3.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 4.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 4.500000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.000000e-05 reg 5.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "\n",
      "best validation accuracy achieved during cross-validation: 0.404 and the alpha, regularization that achieved it: (1e-06, 1000.0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.377000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " **Inline Question** - *True or False*\n",
    "\n",
    " It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    " *Your answer*: True\n",
    "\n",
    " *Your explanation*: If the newly added data point has a large margin i.e the difference between scores of correct and incorrect class plus delta is less than 0, max function wil cap it at 0 in SVM loss.\n",
    " But that won't be the case for softmax as it involves taking exponents of the score. The resultant exponent will be small,but nonetheless would change the sotmax loss.\n",
    "\n",
    ""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 10 Axes>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}